{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/huggingface/peft/blob/main/examples/fp4_finetuning/finetune_fp4_opt_bnb_peft.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "import lightning as pl\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoConfig\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from peft import LoraConfig, get_peft_model, IA3Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn-v0')\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "warnings.filterwarnings(\"ignore\", \".*does not have many workers.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"TheBloke/phi-2-GPTQ\"\n",
    "model_name = \"microsoft/phi-2\"\n",
    "\n",
    "def load_model():\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        # quantization_config=BitsAndBytesConfig(\n",
    "        #     load_in_4bit=True,\n",
    "        #     llm_int8_threshold=6.0,\n",
    "        #     llm_int8_has_fp16_weight=False,\n",
    "        #     bnb_4bit_compute_dtype=torch.float16,\n",
    "        #     bnb_4bit_use_double_quant=True,\n",
    "        #     bnb_4bit_quant_type=\"nf4\",\n",
    "        # ),\n",
    "        torch_dtype=torch.float16,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "\n",
    "\n",
    "    # config = AutoConfig.from_pretrained(model_name, trust_remote_code=True,)\n",
    "    # config.quantization_config['use_exllama'] = False\n",
    "    # config.quantization_config['disable_exllama'] = True\n",
    "    # model = AutoModelForCausalLM.from_pretrained(\n",
    "    #     model_name,\n",
    "    #     torch_dtype=torch.bfloat16,\n",
    "    #     trust_remote_code=True,\n",
    "    #     config=config,\n",
    "    # )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.48it/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "base_model = load_model()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True,)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_model(base_model):\n",
    "    # peft_config = LoraConfig(\n",
    "    #     # task_type=TaskType.TOKEN_CLS, \n",
    "    #     target_modules=[ \"fc2\",  \"Wqkv\",],\n",
    "    #     inference_mode=False, r=4, lora_alpha=4, \n",
    "    #     # lora_dropout=0.1, \n",
    "    #     # bias=\"all\"\n",
    "    # )\n",
    "    peft_config = IA3Config(\n",
    "        target_modules=[ \"fc2\",  \"Wqkv\", 'out_proj', 'fc1'], \n",
    "            feedforward_modules=[\"fc2\", 'fc1', 'out_proj'],\n",
    "            inference_mode=False,\n",
    "    )\n",
    "    model = get_peft_model(base_model, peft_config)\n",
    "    model.config.use_cache = False\n",
    "    return model\n",
    "\n",
    "model = reset_model(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PosixPath('../samples/bletchley_decleration.md'), PosixPath('../samples/cicero_fin1.md'), PosixPath('../samples/disney_appointment.md'), PosixPath('../samples/fake_paper.md'), PosixPath('../samples/fauci_emails.md'), PosixPath('../samples/harvard_announcement_reminders.md'), PosixPath('../samples/how_to_catch_a_liar.md'), PosixPath('../samples/lk-99_end.md'), PosixPath('../samples/lk-99_espanol.md'), PosixPath('../samples/lorem_ipsum.md'), PosixPath('../samples/openai_board_ann.md'), PosixPath('../samples/openai_paper_weak_to_strong.md'), PosixPath('../samples/politics_is_the_mind_killer.md'), PosixPath('../samples/statement_vyKamala_on_passing_of_johnson.md'), PosixPath('../samples/survey_of_rumours.md')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['f', 'title', 'url', 'content'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "MAX_LEN = 400\n",
    "\n",
    "import frontmatter\n",
    "from pathlib import Path\n",
    "sample_files = sorted(Path(\"../samples/\").glob('*.md'))\n",
    "print(sample_files)\n",
    "samples = [{'f':f, **frontmatter.load(f).to_dict()} for f in sample_files]\n",
    "\n",
    "for sample in samples:\n",
    "    assert 'title' in sample, sample['f']\n",
    "    assert 'content' in sample\n",
    "samples[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modified from https://github.dev/huggingface/evaluate/blob/8dfe05784099fb9af55b8e77793205a3b7c86465/measurements/perplexity/perplexity.py#L154\n",
    "import evaluate\n",
    "from evaluate import logging\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def perplexity_compute(\n",
    "    ds, model, tokenizer, batch_size: int = 16, add_start_token: bool = True, device=None, max_length=None\n",
    "):\n",
    "    model = model.to(device)\n",
    "\n",
    "\n",
    "    ds = ds.with_format('pt')\n",
    "    dl = DataLoader(ds, batch_size=1, shuffle=False, num_workers=0, collate_fn=tokenizer.pad, pin_memory=True)\n",
    "    ppls = []\n",
    "    loss_fct = CrossEntropyLoss(reduction=\"none\")\n",
    "    for b in dl:\n",
    "        input_ids = b['input_ids'].to(device)\n",
    "        attention_mask = b['attention_mask'].to(device)\n",
    "        print(attention_mask)\n",
    "\n",
    "        labels = input_ids\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out_logits = model(input_ids=input_ids, attention_mask=attention_mask).logits\n",
    "\n",
    "        shift_logits = out_logits[..., :-1, :].contiguous()\n",
    "        shift_labels = labels[..., 1:].contiguous()\n",
    "        shift_attention_mask_batch = attention_mask[..., 1:].contiguous()\n",
    "\n",
    "        perplexity_batch = torch.exp(\n",
    "            (loss_fct(shift_logits.transpose(1, 2), shift_labels) * shift_attention_mask_batch).sum(1)\n",
    "            / shift_attention_mask_batch.sum(1)\n",
    "        )\n",
    "\n",
    "        ppls += perplexity_batch.tolist()\n",
    "\n",
    "    return {\"perplexities\": ppls, \"mean_perplexity\": torch.tensor(ppls).mean()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perplexity_compute(ds=ds_val, model=model, tokenizer=tokenizer, device='cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lightning helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, tokenizer, ds_val: Dataset):\n",
    "    model.eval();\n",
    "    with torch.no_grad():\n",
    "        with model.disable_adapter():\n",
    "            results = perplexity_compute(ds=ds_val, model=model, tokenizer=tokenizer, device='cuda')\n",
    "        results2 = perplexity_compute(ds=ds_val, model=model, tokenizer=tokenizer, device='cuda')\n",
    "    return dict(before=results['mean_perplexity'].item(), after=results2['mean_perplexity'].item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "def compute_metrics(eval_prediction):\n",
    "    return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainer docs\n",
    "\n",
    "- https://huggingface.co/docs/transformers/v4.36.1/en/main_classes/trainer#transformers.Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 14201)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_LEN, len(sample['content'])//3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tokenize_and_split(examples):\n",
    "    l = len(tokenizer(examples).input_ids[0])\n",
    "    max_len = min(l//3, MAX_LEN) # break into at least 5\n",
    "    max_len = max(max_len, 10)\n",
    "\n",
    "\n",
    "    result = tokenizer(\n",
    "        examples,\n",
    "        # add_special_tokens=False,\n",
    "        truncation=True,\n",
    "        stride=2,\n",
    "        max_length=max_len,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_attention_mask=True,\n",
    "        # return_tensors=\"pt\",\n",
    "    )\n",
    "    return result\n",
    "\n",
    "# s = sample['content']\n",
    "# first_half = s[:len(s)//2]\n",
    "# second_half = s[len(s)//2:]\n",
    "# ds_train = Dataset.from_dict(tokenize_and_split([first_half]))\n",
    "# ds_val = Dataset.from_dict(tokenize_and_split([second_half]))\n",
    "# ds_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8172 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "s = sample['content']\n",
    "\n",
    "d = Dataset.from_dict(tokenize_and_split([s]))\n",
    "d2  = d.train_test_split(test_size=0.5, seed=42)\n",
    "ds_train = d2['train']\n",
    "ds_val = d2['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES']=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_sample(sample):\n",
    "    # device = 'cuda'\n",
    "    # lr = 4e-3\n",
    "    # epochs = 3\n",
    "    # accum_steps = 1\n",
    "    batch_size = 1\n",
    "    verbose = False\n",
    "\n",
    "    s = sample['content']\n",
    "\n",
    "    d = Dataset.from_dict(tokenize_and_split([s]))\n",
    "    d2  = d.train_test_split(test_size=0.5, seed=42)\n",
    "    ds_train = d2['train']\n",
    "    ds_val = d2['test']\n",
    "\n",
    "    \n",
    "    model = reset_model(base_model)\n",
    "    # eval(model, tokenizer, ds_train)\n",
    "\n",
    "    # https://huggingface.co/docs/transformers/v4.36.1/en/main_classes/trainer#transformers.Trainer\n",
    "    trainer = transformers.Trainer(\n",
    "        model=model,\n",
    "        train_dataset=ds_train,\n",
    "        eval_dataset=ds_val,\n",
    "        compute_metrics=compute_metrics, # without this it wont even give val loss\n",
    "        args=transformers.TrainingArguments(\n",
    "            # checkpoint='epoch',\n",
    "            save_strategy='epoch',\n",
    "            label_names=['labels',],\n",
    "            per_device_train_batch_size=batch_size,\n",
    "            gradient_accumulation_steps=3,\n",
    "            warmup_steps=6,\n",
    "            max_steps=20,\n",
    "            learning_rate=3e-3,\n",
    "            fp16=True,\n",
    "            logging_steps=1,\n",
    "            output_dir=\"outputs\",\n",
    "            log_level='error',\n",
    "            # do_eval=True,\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            eval_steps=1,\n",
    "            load_best_model_at_end=True,\n",
    "            \n",
    "            # disable_tqdm=not verbose,\n",
    "        ),\n",
    "        data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    "    )\n",
    "    trainer._signature_columns = ['input_ids', 'attention_mask', 'labels',]\n",
    "    model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "    train_output = trainer.train()\n",
    "\n",
    "    df_hist = pd.DataFrame(trainer.state.log_history)\n",
    "    df_hist_epoch = df_hist.groupby('epoch').last().drop(columns=['step'])\n",
    "    df_hist_step = df_hist.set_index('step').dropna(thresh=2, axis=1)\n",
    "    if verbose:\n",
    "        df_hist_epoch['loss'].plot()\n",
    "        plt.twinx()\n",
    "        df_hist_epoch['eval_loss'].plot(c='b', label='eval')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    result_train = {f'train/{k}':v for k,v in eval(model, tokenizer, ds_train).items()}\n",
    "    result = eval(model, tokenizer, ds_val)\n",
    "    result['hist'] = df_hist_epoch\n",
    "    result.update(result_train)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "  5%|▌         | 1/20 [00:06<02:03,  6.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8539, 'learning_rate': 0.0005, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                              \n",
      "  5%|▌         | 1/20 [00:08<02:03,  6.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.881439447402954, 'eval_runtime': 1.8561, 'eval_samples_per_second': 1.078, 'eval_steps_per_second': 0.539, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      " 10%|█         | 2/20 [00:12<01:46,  5.94s/it]/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8546, 'learning_rate': 0.001, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                              \n",
      " 10%|█         | 2/20 [00:13<01:46,  5.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.875718593597412, 'eval_runtime': 1.8505, 'eval_samples_per_second': 1.081, 'eval_steps_per_second': 0.54, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      " 15%|█▌        | 3/20 [00:17<01:37,  5.74s/it]/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8479, 'learning_rate': 0.0015, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                              \n",
      " 15%|█▌        | 3/20 [00:19<01:37,  5.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.8696751594543457, 'eval_runtime': 1.8533, 'eval_samples_per_second': 1.079, 'eval_steps_per_second': 0.54, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      " 20%|██        | 4/20 [00:23<01:30,  5.64s/it]/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8419, 'learning_rate': 0.002, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                              \n",
      " 20%|██        | 4/20 [00:24<01:30,  5.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.8617634773254395, 'eval_runtime': 1.8551, 'eval_samples_per_second': 1.078, 'eval_steps_per_second': 0.539, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      " 25%|██▌       | 5/20 [00:28<01:24,  5.61s/it]/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.831, 'learning_rate': 0.0025, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                              \n",
      " 25%|██▌       | 5/20 [00:30<01:24,  5.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.8497085571289062, 'eval_runtime': 1.8556, 'eval_samples_per_second': 1.078, 'eval_steps_per_second': 0.539, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      " 30%|███       | 6/20 [00:34<01:18,  5.59s/it]/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8073, 'learning_rate': 0.003, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                              \n",
      " 30%|███       | 6/20 [00:36<01:18,  5.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.8383166790008545, 'eval_runtime': 1.8605, 'eval_samples_per_second': 1.075, 'eval_steps_per_second': 0.537, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      " 35%|███▌      | 7/20 [00:39<01:12,  5.59s/it]/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.788, 'learning_rate': 0.002785714285714286, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                              \n",
      " 35%|███▌      | 7/20 [00:41<01:12,  5.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.8310322761535645, 'eval_runtime': 1.8517, 'eval_samples_per_second': 1.08, 'eval_steps_per_second': 0.54, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      " 40%|████      | 8/20 [00:45<01:06,  5.58s/it]/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7605, 'learning_rate': 0.0025714285714285713, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                              \n",
      " 40%|████      | 8/20 [00:47<01:06,  5.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.8236138820648193, 'eval_runtime': 1.8571, 'eval_samples_per_second': 1.077, 'eval_steps_per_second': 0.538, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      " 45%|████▌     | 9/20 [00:50<01:01,  5.57s/it]/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7438, 'learning_rate': 0.002357142857142857, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                              \n",
      " 45%|████▌     | 9/20 [00:52<01:01,  5.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.818512201309204, 'eval_runtime': 1.8553, 'eval_samples_per_second': 1.078, 'eval_steps_per_second': 0.539, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      " 50%|█████     | 10/20 [00:56<00:55,  5.55s/it]/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7233, 'learning_rate': 0.002142857142857143, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      " 50%|█████     | 10/20 [00:58<00:55,  5.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.8183467388153076, 'eval_runtime': 1.8538, 'eval_samples_per_second': 1.079, 'eval_steps_per_second': 0.539, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      " 55%|█████▌    | 11/20 [01:01<00:49,  5.54s/it]/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7043, 'learning_rate': 0.0019285714285714288, 'epoch': 11.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      " 55%|█████▌    | 11/20 [01:03<00:49,  5.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.8191444873809814, 'eval_runtime': 1.8589, 'eval_samples_per_second': 1.076, 'eval_steps_per_second': 0.538, 'epoch': 11.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      " 60%|██████    | 12/20 [01:07<00:44,  5.59s/it]/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6918, 'learning_rate': 0.0017142857142857142, 'epoch': 12.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      " 60%|██████    | 12/20 [01:09<00:44,  5.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.8198442459106445, 'eval_runtime': 1.8602, 'eval_samples_per_second': 1.075, 'eval_steps_per_second': 0.538, 'epoch': 12.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      " 65%|██████▌   | 13/20 [01:13<00:39,  5.58s/it]/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6893, 'learning_rate': 0.0015, 'epoch': 13.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      " 65%|██████▌   | 13/20 [01:14<00:39,  5.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.8221230506896973, 'eval_runtime': 1.8565, 'eval_samples_per_second': 1.077, 'eval_steps_per_second': 0.539, 'epoch': 13.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      " 70%|███████   | 14/20 [01:18<00:33,  5.57s/it]/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6683, 'learning_rate': 0.0012857142857142856, 'epoch': 14.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      " 70%|███████   | 14/20 [01:20<00:33,  5.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.820427417755127, 'eval_runtime': 1.8649, 'eval_samples_per_second': 1.072, 'eval_steps_per_second': 0.536, 'epoch': 14.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      " 75%|███████▌  | 15/20 [01:24<00:27,  5.56s/it]/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6568, 'learning_rate': 0.0010714285714285715, 'epoch': 15.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      " 75%|███████▌  | 15/20 [01:26<00:27,  5.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.8205204010009766, 'eval_runtime': 1.8536, 'eval_samples_per_second': 1.079, 'eval_steps_per_second': 0.539, 'epoch': 15.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      " 80%|████████  | 16/20 [01:29<00:22,  5.54s/it]/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6538, 'learning_rate': 0.0008571428571428571, 'epoch': 16.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      " 80%|████████  | 16/20 [01:31<00:22,  5.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.8183021545410156, 'eval_runtime': 1.8539, 'eval_samples_per_second': 1.079, 'eval_steps_per_second': 0.539, 'epoch': 16.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      " 85%|████████▌ | 17/20 [01:35<00:16,  5.53s/it]/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6506, 'learning_rate': 0.0006428571428571428, 'epoch': 17.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      " 85%|████████▌ | 17/20 [01:37<00:16,  5.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.8169333934783936, 'eval_runtime': 1.8602, 'eval_samples_per_second': 1.075, 'eval_steps_per_second': 0.538, 'epoch': 17.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      " 90%|█████████ | 18/20 [01:40<00:11,  5.53s/it]/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6389, 'learning_rate': 0.00042857142857142855, 'epoch': 18.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      " 90%|█████████ | 18/20 [01:42<00:11,  5.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.8156843185424805, 'eval_runtime': 1.8561, 'eval_samples_per_second': 1.078, 'eval_steps_per_second': 0.539, 'epoch': 18.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      " 95%|█████████▌| 19/20 [01:46<00:05,  5.54s/it]/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6351, 'learning_rate': 0.00021428571428571427, 'epoch': 19.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      " 95%|█████████▌| 19/20 [01:48<00:05,  5.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.81685209274292, 'eval_runtime': 1.8528, 'eval_samples_per_second': 1.079, 'eval_steps_per_second': 0.54, 'epoch': 19.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "100%|██████████| 20/20 [01:51<00:00,  5.53s/it]/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6306, 'learning_rate': 0.0, 'epoch': 20.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      "100%|██████████| 20/20 [01:53<00:00,  5.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.8111038208007812, 'eval_runtime': 1.8573, 'eval_samples_per_second': 1.077, 'eval_steps_per_second': 0.538, 'epoch': 20.0}\n",
      "{'train_runtime': 113.699, 'train_samples_per_second': 1.055, 'train_steps_per_second': 0.176, 'train_loss': 0.733573117852211, 'epoch': 20.0}\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1]], device='cuda:0')\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1]], device='cuda:0')\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "blechley declaration\n",
      "{'before': 17.86233901977539, 'after': 16.78615951538086}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n",
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.5961, 'learning_rate': 0.0005, 'epoch': 1.0}\n",
      "{'eval_loss': 4.274234771728516, 'eval_runtime': 2.6102, 'eval_samples_per_second': 2.299, 'eval_steps_per_second': 0.383, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.6001, 'learning_rate': 0.001, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.262000560760498, 'eval_runtime': 2.239, 'eval_samples_per_second': 2.68, 'eval_steps_per_second': 0.447, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.5752, 'learning_rate': 0.0015, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.248987197875977, 'eval_runtime': 2.1915, 'eval_samples_per_second': 2.738, 'eval_steps_per_second': 0.456, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.5556, 'learning_rate': 0.002, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.232681751251221, 'eval_runtime': 2.2127, 'eval_samples_per_second': 2.712, 'eval_steps_per_second': 0.452, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.5043, 'learning_rate': 0.0025, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.215490341186523, 'eval_runtime': 2.2104, 'eval_samples_per_second': 2.714, 'eval_steps_per_second': 0.452, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.4729, 'learning_rate': 0.003, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.199138641357422, 'eval_runtime': 2.1961, 'eval_samples_per_second': 2.732, 'eval_steps_per_second': 0.455, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.4295, 'learning_rate': 0.002785714285714286, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.1823601722717285, 'eval_runtime': 2.2026, 'eval_samples_per_second': 2.724, 'eval_steps_per_second': 0.454, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.3655, 'learning_rate': 0.0025714285714285713, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.170837879180908, 'eval_runtime': 2.1987, 'eval_samples_per_second': 2.729, 'eval_steps_per_second': 0.455, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.3208, 'learning_rate': 0.002357142857142857, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.162167072296143, 'eval_runtime': 2.2001, 'eval_samples_per_second': 2.727, 'eval_steps_per_second': 0.455, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.2858, 'learning_rate': 0.002142857142857143, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.1558837890625, 'eval_runtime': 2.2045, 'eval_samples_per_second': 2.722, 'eval_steps_per_second': 0.454, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.2493, 'learning_rate': 0.0019285714285714288, 'epoch': 11.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.152297496795654, 'eval_runtime': 2.1987, 'eval_samples_per_second': 2.729, 'eval_steps_per_second': 0.455, 'epoch': 11.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.2295, 'learning_rate': 0.0017142857142857142, 'epoch': 12.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.14851713180542, 'eval_runtime': 2.1963, 'eval_samples_per_second': 2.732, 'eval_steps_per_second': 0.455, 'epoch': 12.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.2075, 'learning_rate': 0.0015, 'epoch': 13.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.145945072174072, 'eval_runtime': 2.1968, 'eval_samples_per_second': 2.731, 'eval_steps_per_second': 0.455, 'epoch': 13.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.1733, 'learning_rate': 0.0012857142857142856, 'epoch': 14.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.144321918487549, 'eval_runtime': 2.1946, 'eval_samples_per_second': 2.734, 'eval_steps_per_second': 0.456, 'epoch': 14.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.1486, 'learning_rate': 0.0010714285714285715, 'epoch': 15.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.142940044403076, 'eval_runtime': 2.2034, 'eval_samples_per_second': 2.723, 'eval_steps_per_second': 0.454, 'epoch': 15.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.172, 'learning_rate': 0.0008571428571428571, 'epoch': 16.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.142568111419678, 'eval_runtime': 2.2068, 'eval_samples_per_second': 2.719, 'eval_steps_per_second': 0.453, 'epoch': 16.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.1096, 'learning_rate': 0.0006428571428571428, 'epoch': 17.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.142303943634033, 'eval_runtime': 2.1981, 'eval_samples_per_second': 2.73, 'eval_steps_per_second': 0.455, 'epoch': 17.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.1034, 'learning_rate': 0.00042857142857142855, 'epoch': 18.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.140881061553955, 'eval_runtime': 2.1963, 'eval_samples_per_second': 2.732, 'eval_steps_per_second': 0.455, 'epoch': 18.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.1114, 'learning_rate': 0.00021428571428571427, 'epoch': 19.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.141260147094727, 'eval_runtime': 2.1986, 'eval_samples_per_second': 2.729, 'eval_steps_per_second': 0.455, 'epoch': 19.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.0975, 'learning_rate': 0.0, 'epoch': 20.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.140285491943359, 'eval_runtime': 2.1994, 'eval_samples_per_second': 2.728, 'eval_steps_per_second': 0.455, 'epoch': 20.0}\n",
      "{'train_runtime': 261.6994, 'train_samples_per_second': 0.459, 'train_steps_per_second': 0.076, 'train_loss': 4.315399408340454, 'epoch': 20.0}\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "ibois, Philippe (2012-06-03).\n",
      "{'before': 72.71493530273438, 'after': 63.473968505859375}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n",
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7009, 'learning_rate': 0.0005, 'epoch': 1.0}\n",
      "{'eval_loss': 3.662641763687134, 'eval_runtime': 1.7981, 'eval_samples_per_second': 1.112, 'eval_steps_per_second': 0.556, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7008, 'learning_rate': 0.001, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.662975311279297, 'eval_runtime': 1.7937, 'eval_samples_per_second': 1.115, 'eval_steps_per_second': 0.558, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6939, 'learning_rate': 0.0015, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.6626994609832764, 'eval_runtime': 1.7941, 'eval_samples_per_second': 1.115, 'eval_steps_per_second': 0.557, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6772, 'learning_rate': 0.002, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.658553123474121, 'eval_runtime': 1.7937, 'eval_samples_per_second': 1.115, 'eval_steps_per_second': 0.558, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6631, 'learning_rate': 0.0025, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.653069019317627, 'eval_runtime': 1.7937, 'eval_samples_per_second': 1.115, 'eval_steps_per_second': 0.558, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6382, 'learning_rate': 0.003, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.6443142890930176, 'eval_runtime': 1.7945, 'eval_samples_per_second': 1.114, 'eval_steps_per_second': 0.557, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6186, 'learning_rate': 0.002785714285714286, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.638564109802246, 'eval_runtime': 1.7937, 'eval_samples_per_second': 1.115, 'eval_steps_per_second': 0.557, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5862, 'learning_rate': 0.0025714285714285713, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.634767532348633, 'eval_runtime': 1.7935, 'eval_samples_per_second': 1.115, 'eval_steps_per_second': 0.558, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5627, 'learning_rate': 0.002357142857142857, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.627084732055664, 'eval_runtime': 1.7925, 'eval_samples_per_second': 1.116, 'eval_steps_per_second': 0.558, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5433, 'learning_rate': 0.002142857142857143, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.624904155731201, 'eval_runtime': 1.794, 'eval_samples_per_second': 1.115, 'eval_steps_per_second': 0.557, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5227, 'learning_rate': 0.0019285714285714288, 'epoch': 11.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.621706247329712, 'eval_runtime': 1.7934, 'eval_samples_per_second': 1.115, 'eval_steps_per_second': 0.558, 'epoch': 11.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5124, 'learning_rate': 0.0017142857142857142, 'epoch': 12.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.625009536743164, 'eval_runtime': 1.7934, 'eval_samples_per_second': 1.115, 'eval_steps_per_second': 0.558, 'epoch': 12.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4915, 'learning_rate': 0.0015, 'epoch': 13.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.618410110473633, 'eval_runtime': 1.7931, 'eval_samples_per_second': 1.115, 'eval_steps_per_second': 0.558, 'epoch': 13.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4786, 'learning_rate': 0.0012857142857142856, 'epoch': 14.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.6174843311309814, 'eval_runtime': 1.7932, 'eval_samples_per_second': 1.115, 'eval_steps_per_second': 0.558, 'epoch': 14.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4708, 'learning_rate': 0.0010714285714285715, 'epoch': 15.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.6127610206604004, 'eval_runtime': 1.794, 'eval_samples_per_second': 1.115, 'eval_steps_per_second': 0.557, 'epoch': 15.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4566, 'learning_rate': 0.0008571428571428571, 'epoch': 16.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.617872714996338, 'eval_runtime': 1.7934, 'eval_samples_per_second': 1.115, 'eval_steps_per_second': 0.558, 'epoch': 16.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.449, 'learning_rate': 0.0006428571428571428, 'epoch': 17.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.61689829826355, 'eval_runtime': 1.793, 'eval_samples_per_second': 1.115, 'eval_steps_per_second': 0.558, 'epoch': 17.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4465, 'learning_rate': 0.00042857142857142855, 'epoch': 18.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.6198794841766357, 'eval_runtime': 1.7954, 'eval_samples_per_second': 1.114, 'eval_steps_per_second': 0.557, 'epoch': 18.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4353, 'learning_rate': 0.00021428571428571427, 'epoch': 19.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.616215467453003, 'eval_runtime': 1.7956, 'eval_samples_per_second': 1.114, 'eval_steps_per_second': 0.557, 'epoch': 19.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4332, 'learning_rate': 0.0, 'epoch': 20.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.623260259628296, 'eval_runtime': 1.7942, 'eval_samples_per_second': 1.115, 'eval_steps_per_second': 0.557, 'epoch': 20.0}\n",
      "{'train_runtime': 106.9759, 'train_samples_per_second': 1.122, 'train_steps_per_second': 0.187, 'train_loss': 0.5540790364146233, 'epoch': 20.0}\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1]], device='cuda:0')\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1]], device='cuda:0')\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1]], device='cuda:0')\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1]], device='cuda:0')\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1]], device='cuda:0')\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1]], device='cuda:0')\n",
      "disney appointment\n",
      "{'before': 118.5029067993164, 'after': 110.34452819824219}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n",
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6748, 'learning_rate': 0.0005, 'epoch': 1.0}\n",
      "{'eval_loss': 2.0232930183410645, 'eval_runtime': 1.765, 'eval_samples_per_second': 1.133, 'eval_steps_per_second': 0.567, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6777, 'learning_rate': 0.001, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.0197839736938477, 'eval_runtime': 1.7657, 'eval_samples_per_second': 1.133, 'eval_steps_per_second': 0.566, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6718, 'learning_rate': 0.0015, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.014660358428955, 'eval_runtime': 1.7634, 'eval_samples_per_second': 1.134, 'eval_steps_per_second': 0.567, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6535, 'learning_rate': 0.002, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.0122783184051514, 'eval_runtime': 1.763, 'eval_samples_per_second': 1.134, 'eval_steps_per_second': 0.567, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6451, 'learning_rate': 0.0025, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.0040552616119385, 'eval_runtime': 1.7624, 'eval_samples_per_second': 1.135, 'eval_steps_per_second': 0.567, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6146, 'learning_rate': 0.003, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.9945719242095947, 'eval_runtime': 1.7631, 'eval_samples_per_second': 1.134, 'eval_steps_per_second': 0.567, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5808, 'learning_rate': 0.002785714285714286, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.986754298210144, 'eval_runtime': 1.7637, 'eval_samples_per_second': 1.134, 'eval_steps_per_second': 0.567, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5565, 'learning_rate': 0.0025714285714285713, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.9828104972839355, 'eval_runtime': 1.7637, 'eval_samples_per_second': 1.134, 'eval_steps_per_second': 0.567, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.521, 'learning_rate': 0.002357142857142857, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.9781702756881714, 'eval_runtime': 1.7642, 'eval_samples_per_second': 1.134, 'eval_steps_per_second': 0.567, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4827, 'learning_rate': 0.002142857142857143, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.976542592048645, 'eval_runtime': 1.7651, 'eval_samples_per_second': 1.133, 'eval_steps_per_second': 0.567, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4783, 'learning_rate': 0.0019285714285714288, 'epoch': 11.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.9733855724334717, 'eval_runtime': 1.7648, 'eval_samples_per_second': 1.133, 'eval_steps_per_second': 0.567, 'epoch': 11.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4523, 'learning_rate': 0.0017142857142857142, 'epoch': 12.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.976525068283081, 'eval_runtime': 1.7651, 'eval_samples_per_second': 1.133, 'eval_steps_per_second': 0.567, 'epoch': 12.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4428, 'learning_rate': 0.0015, 'epoch': 13.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.9778211116790771, 'eval_runtime': 1.7642, 'eval_samples_per_second': 1.134, 'eval_steps_per_second': 0.567, 'epoch': 13.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4254, 'learning_rate': 0.0012857142857142856, 'epoch': 14.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.976356029510498, 'eval_runtime': 1.7655, 'eval_samples_per_second': 1.133, 'eval_steps_per_second': 0.566, 'epoch': 14.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4067, 'learning_rate': 0.0010714285714285715, 'epoch': 15.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.9761137962341309, 'eval_runtime': 1.7658, 'eval_samples_per_second': 1.133, 'eval_steps_per_second': 0.566, 'epoch': 15.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3959, 'learning_rate': 0.0008571428571428571, 'epoch': 16.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.9756845235824585, 'eval_runtime': 1.7636, 'eval_samples_per_second': 1.134, 'eval_steps_per_second': 0.567, 'epoch': 16.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.382, 'learning_rate': 0.0006428571428571428, 'epoch': 17.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.976212739944458, 'eval_runtime': 1.7644, 'eval_samples_per_second': 1.134, 'eval_steps_per_second': 0.567, 'epoch': 17.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3788, 'learning_rate': 0.00042857142857142855, 'epoch': 18.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.9746170043945312, 'eval_runtime': 1.7627, 'eval_samples_per_second': 1.135, 'eval_steps_per_second': 0.567, 'epoch': 18.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3734, 'learning_rate': 0.00021428571428571427, 'epoch': 19.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.9778207540512085, 'eval_runtime': 1.7635, 'eval_samples_per_second': 1.134, 'eval_steps_per_second': 0.567, 'epoch': 19.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3668, 'learning_rate': 0.0, 'epoch': 20.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.9766753911972046, 'eval_runtime': 1.7631, 'eval_samples_per_second': 1.134, 'eval_steps_per_second': 0.567, 'epoch': 20.0}\n",
      "{'train_runtime': 105.642, 'train_samples_per_second': 1.136, 'train_steps_per_second': 0.189, 'train_loss': 0.5090549781918525, 'epoch': 20.0}\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:0')\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:0')\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:0')\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:0')\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:0')\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:0')\n",
      "fake ai hoax paper\n",
      "{'before': 7.74808406829834, 'after': 7.473690032958984}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n",
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.2525, 'learning_rate': 0.0005, 'epoch': 1.0}\n",
      "{'eval_loss': 3.050722360610962, 'eval_runtime': 2.1997, 'eval_samples_per_second': 2.728, 'eval_steps_per_second': 0.455, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.2068, 'learning_rate': 0.001, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.044886589050293, 'eval_runtime': 2.2023, 'eval_samples_per_second': 2.724, 'eval_steps_per_second': 0.454, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.1089, 'learning_rate': 0.0015, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.0375711917877197, 'eval_runtime': 2.1969, 'eval_samples_per_second': 2.731, 'eval_steps_per_second': 0.455, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.1984, 'learning_rate': 0.002, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.0268099308013916, 'eval_runtime': 2.1969, 'eval_samples_per_second': 2.731, 'eval_steps_per_second': 0.455, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.1795, 'learning_rate': 0.0025, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.0156776905059814, 'eval_runtime': 2.2075, 'eval_samples_per_second': 2.718, 'eval_steps_per_second': 0.453, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.0066, 'learning_rate': 0.003, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.0008761882781982, 'eval_runtime': 2.2109, 'eval_samples_per_second': 2.714, 'eval_steps_per_second': 0.452, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.048, 'learning_rate': 0.002785714285714286, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.9874343872070312, 'eval_runtime': 2.2002, 'eval_samples_per_second': 2.727, 'eval_steps_per_second': 0.455, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.9512, 'learning_rate': 0.0025714285714285713, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.9755964279174805, 'eval_runtime': 2.2025, 'eval_samples_per_second': 2.724, 'eval_steps_per_second': 0.454, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.8015, 'learning_rate': 0.002357142857142857, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.967024803161621, 'eval_runtime': 2.197, 'eval_samples_per_second': 2.731, 'eval_steps_per_second': 0.455, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.8529, 'learning_rate': 0.002142857142857143, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.9581174850463867, 'eval_runtime': 2.2057, 'eval_samples_per_second': 2.72, 'eval_steps_per_second': 0.453, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7958, 'learning_rate': 0.0019285714285714288, 'epoch': 11.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.9509658813476562, 'eval_runtime': 2.2133, 'eval_samples_per_second': 2.711, 'eval_steps_per_second': 0.452, 'epoch': 11.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7576, 'learning_rate': 0.0017142857142857142, 'epoch': 12.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.9437856674194336, 'eval_runtime': 2.1974, 'eval_samples_per_second': 2.73, 'eval_steps_per_second': 0.455, 'epoch': 12.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.73, 'learning_rate': 0.0015, 'epoch': 13.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.940640449523926, 'eval_runtime': 2.2039, 'eval_samples_per_second': 2.722, 'eval_steps_per_second': 0.454, 'epoch': 13.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6723, 'learning_rate': 0.0012857142857142856, 'epoch': 14.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.937736749649048, 'eval_runtime': 2.1983, 'eval_samples_per_second': 2.729, 'eval_steps_per_second': 0.455, 'epoch': 14.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6535, 'learning_rate': 0.0010714285714285715, 'epoch': 15.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.9327847957611084, 'eval_runtime': 2.204, 'eval_samples_per_second': 2.722, 'eval_steps_per_second': 0.454, 'epoch': 15.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6328, 'learning_rate': 0.0008571428571428571, 'epoch': 16.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.9306774139404297, 'eval_runtime': 2.1963, 'eval_samples_per_second': 2.732, 'eval_steps_per_second': 0.455, 'epoch': 16.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5886, 'learning_rate': 0.0006428571428571428, 'epoch': 17.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.928274393081665, 'eval_runtime': 2.1961, 'eval_samples_per_second': 2.732, 'eval_steps_per_second': 0.455, 'epoch': 17.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6005, 'learning_rate': 0.00042857142857142855, 'epoch': 18.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.9272444248199463, 'eval_runtime': 2.2033, 'eval_samples_per_second': 2.723, 'eval_steps_per_second': 0.454, 'epoch': 18.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5738, 'learning_rate': 0.00021428571428571427, 'epoch': 19.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.925577163696289, 'eval_runtime': 2.1969, 'eval_samples_per_second': 2.731, 'eval_steps_per_second': 0.455, 'epoch': 19.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5716, 'learning_rate': 0.0, 'epoch': 20.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.9260060787200928, 'eval_runtime': 2.199, 'eval_samples_per_second': 2.729, 'eval_steps_per_second': 0.455, 'epoch': 20.0}\n",
      "{'train_runtime': 193.4235, 'train_samples_per_second': 0.62, 'train_steps_per_second': 0.103, 'train_loss': 2.8591257452964784, 'epoch': 20.0}\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "buzzfeed foi fauci emails 2023\n",
      "{'before': 23.320093154907227, 'after': 20.782445907592773}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n",
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8462, 'learning_rate': 0.0005, 'epoch': 1.0}\n",
      "{'eval_loss': 3.5164437294006348, 'eval_runtime': 1.82, 'eval_samples_per_second': 1.099, 'eval_steps_per_second': 0.549, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8528, 'learning_rate': 0.001, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.513643503189087, 'eval_runtime': 1.8163, 'eval_samples_per_second': 1.101, 'eval_steps_per_second': 0.551, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8405, 'learning_rate': 0.0015, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.5125553607940674, 'eval_runtime': 1.8155, 'eval_samples_per_second': 1.102, 'eval_steps_per_second': 0.551, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8305, 'learning_rate': 0.002, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.5090177059173584, 'eval_runtime': 1.8186, 'eval_samples_per_second': 1.1, 'eval_steps_per_second': 0.55, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8126, 'learning_rate': 0.0025, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.5054893493652344, 'eval_runtime': 1.8166, 'eval_samples_per_second': 1.101, 'eval_steps_per_second': 0.55, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7924, 'learning_rate': 0.003, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.5034797191619873, 'eval_runtime': 1.823, 'eval_samples_per_second': 1.097, 'eval_steps_per_second': 0.549, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7643, 'learning_rate': 0.002785714285714286, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.499797821044922, 'eval_runtime': 1.8163, 'eval_samples_per_second': 1.101, 'eval_steps_per_second': 0.551, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7377, 'learning_rate': 0.0025714285714285713, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.4974937438964844, 'eval_runtime': 1.8173, 'eval_samples_per_second': 1.101, 'eval_steps_per_second': 0.55, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7107, 'learning_rate': 0.002357142857142857, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.498356819152832, 'eval_runtime': 1.817, 'eval_samples_per_second': 1.101, 'eval_steps_per_second': 0.55, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.693, 'learning_rate': 0.002142857142857143, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.4970808029174805, 'eval_runtime': 1.8175, 'eval_samples_per_second': 1.1, 'eval_steps_per_second': 0.55, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.668, 'learning_rate': 0.0019285714285714288, 'epoch': 11.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.501178741455078, 'eval_runtime': 1.8168, 'eval_samples_per_second': 1.101, 'eval_steps_per_second': 0.55, 'epoch': 11.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6525, 'learning_rate': 0.0017142857142857142, 'epoch': 12.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.4966859817504883, 'eval_runtime': 1.8164, 'eval_samples_per_second': 1.101, 'eval_steps_per_second': 0.551, 'epoch': 12.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6409, 'learning_rate': 0.0015, 'epoch': 13.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.499734401702881, 'eval_runtime': 1.8235, 'eval_samples_per_second': 1.097, 'eval_steps_per_second': 0.548, 'epoch': 13.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6271, 'learning_rate': 0.0012857142857142856, 'epoch': 14.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.503204822540283, 'eval_runtime': 1.8156, 'eval_samples_per_second': 1.102, 'eval_steps_per_second': 0.551, 'epoch': 14.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6175, 'learning_rate': 0.0010714285714285715, 'epoch': 15.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.502041816711426, 'eval_runtime': 1.8175, 'eval_samples_per_second': 1.1, 'eval_steps_per_second': 0.55, 'epoch': 15.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6044, 'learning_rate': 0.0008571428571428571, 'epoch': 16.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.503654956817627, 'eval_runtime': 1.8214, 'eval_samples_per_second': 1.098, 'eval_steps_per_second': 0.549, 'epoch': 16.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5976, 'learning_rate': 0.0006428571428571428, 'epoch': 17.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.502300262451172, 'eval_runtime': 1.8164, 'eval_samples_per_second': 1.101, 'eval_steps_per_second': 0.551, 'epoch': 17.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5934, 'learning_rate': 0.00042857142857142855, 'epoch': 18.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.5038514137268066, 'eval_runtime': 1.8159, 'eval_samples_per_second': 1.101, 'eval_steps_per_second': 0.551, 'epoch': 18.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5904, 'learning_rate': 0.00021428571428571427, 'epoch': 19.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.504033088684082, 'eval_runtime': 1.8167, 'eval_samples_per_second': 1.101, 'eval_steps_per_second': 0.55, 'epoch': 19.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5868, 'learning_rate': 0.0, 'epoch': 20.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.5032901763916016, 'eval_runtime': 1.8163, 'eval_samples_per_second': 1.101, 'eval_steps_per_second': 0.551, 'epoch': 20.0}\n",
      "{'train_runtime': 107.9577, 'train_samples_per_second': 1.112, 'train_steps_per_second': 0.185, 'train_loss': 0.7029642164707184, 'epoch': 20.0}\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "harvard announcment caplain israel hamas\n",
      "{'before': 45.4366569519043, 'after': 44.633602142333984}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n",
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8518, 'learning_rate': 0.0005, 'epoch': 1.0}\n",
      "{'eval_loss': 3.334834575653076, 'eval_runtime': 1.8533, 'eval_samples_per_second': 1.079, 'eval_steps_per_second': 0.54, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8523, 'learning_rate': 0.001, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.328137159347534, 'eval_runtime': 1.8607, 'eval_samples_per_second': 1.075, 'eval_steps_per_second': 0.537, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8447, 'learning_rate': 0.0015, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.3206543922424316, 'eval_runtime': 1.8569, 'eval_samples_per_second': 1.077, 'eval_steps_per_second': 0.539, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8334, 'learning_rate': 0.002, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.303365468978882, 'eval_runtime': 1.8576, 'eval_samples_per_second': 1.077, 'eval_steps_per_second': 0.538, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8172, 'learning_rate': 0.0025, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.2859046459198, 'eval_runtime': 1.8533, 'eval_samples_per_second': 1.079, 'eval_steps_per_second': 0.54, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7944, 'learning_rate': 0.003, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.2634775638580322, 'eval_runtime': 1.853, 'eval_samples_per_second': 1.079, 'eval_steps_per_second': 0.54, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7678, 'learning_rate': 0.002785714285714286, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.243778705596924, 'eval_runtime': 1.8522, 'eval_samples_per_second': 1.08, 'eval_steps_per_second': 0.54, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7379, 'learning_rate': 0.0025714285714285713, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.2242088317871094, 'eval_runtime': 1.8535, 'eval_samples_per_second': 1.079, 'eval_steps_per_second': 0.54, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7185, 'learning_rate': 0.002357142857142857, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.2073123455047607, 'eval_runtime': 1.8527, 'eval_samples_per_second': 1.08, 'eval_steps_per_second': 0.54, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7034, 'learning_rate': 0.002142857142857143, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.195540428161621, 'eval_runtime': 1.8522, 'eval_samples_per_second': 1.08, 'eval_steps_per_second': 0.54, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6838, 'learning_rate': 0.0019285714285714288, 'epoch': 11.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.1888558864593506, 'eval_runtime': 1.8671, 'eval_samples_per_second': 1.071, 'eval_steps_per_second': 0.536, 'epoch': 11.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6664, 'learning_rate': 0.0017142857142857142, 'epoch': 12.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.1770687103271484, 'eval_runtime': 1.864, 'eval_samples_per_second': 1.073, 'eval_steps_per_second': 0.536, 'epoch': 12.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6472, 'learning_rate': 0.0015, 'epoch': 13.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.169701337814331, 'eval_runtime': 1.8551, 'eval_samples_per_second': 1.078, 'eval_steps_per_second': 0.539, 'epoch': 13.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6392, 'learning_rate': 0.0012857142857142856, 'epoch': 14.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.164093017578125, 'eval_runtime': 1.8541, 'eval_samples_per_second': 1.079, 'eval_steps_per_second': 0.539, 'epoch': 14.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6247, 'learning_rate': 0.0010714285714285715, 'epoch': 15.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.159454822540283, 'eval_runtime': 1.854, 'eval_samples_per_second': 1.079, 'eval_steps_per_second': 0.539, 'epoch': 15.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.614, 'learning_rate': 0.0008571428571428571, 'epoch': 16.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.1564927101135254, 'eval_runtime': 1.8545, 'eval_samples_per_second': 1.078, 'eval_steps_per_second': 0.539, 'epoch': 16.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6129, 'learning_rate': 0.0006428571428571428, 'epoch': 17.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.157731056213379, 'eval_runtime': 1.8547, 'eval_samples_per_second': 1.078, 'eval_steps_per_second': 0.539, 'epoch': 17.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6011, 'learning_rate': 0.00042857142857142855, 'epoch': 18.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.1546175479888916, 'eval_runtime': 1.8561, 'eval_samples_per_second': 1.078, 'eval_steps_per_second': 0.539, 'epoch': 18.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5967, 'learning_rate': 0.00021428571428571427, 'epoch': 19.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.151501178741455, 'eval_runtime': 1.8574, 'eval_samples_per_second': 1.077, 'eval_steps_per_second': 0.538, 'epoch': 19.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5928, 'learning_rate': 0.0, 'epoch': 20.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.1531982421875, 'eval_runtime': 1.8559, 'eval_samples_per_second': 1.078, 'eval_steps_per_second': 0.539, 'epoch': 20.0}\n",
      "{'train_runtime': 109.9731, 'train_samples_per_second': 1.091, 'train_steps_per_second': 0.182, 'train_loss': 0.7100029855966568, 'epoch': 20.0}\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "How to Catch an AI Liar\n",
      "{'before': 28.927478790283203, 'after': 23.912349700927734}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n",
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6872, 'learning_rate': 0.0005, 'epoch': 1.0}\n",
      "{'eval_loss': 2.692056179046631, 'eval_runtime': 2.1028, 'eval_samples_per_second': 2.378, 'eval_steps_per_second': 0.476, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7332, 'learning_rate': 0.001, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.6879210472106934, 'eval_runtime': 2.0896, 'eval_samples_per_second': 2.393, 'eval_steps_per_second': 0.479, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7256, 'learning_rate': 0.0015, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.682523727416992, 'eval_runtime': 2.0722, 'eval_samples_per_second': 2.413, 'eval_steps_per_second': 0.483, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.641, 'learning_rate': 0.002, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.675121307373047, 'eval_runtime': 2.0802, 'eval_samples_per_second': 2.404, 'eval_steps_per_second': 0.481, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5249, 'learning_rate': 0.0025, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.664735794067383, 'eval_runtime': 2.0902, 'eval_samples_per_second': 2.392, 'eval_steps_per_second': 0.478, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.609, 'learning_rate': 0.003, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.6542813777923584, 'eval_runtime': 2.0923, 'eval_samples_per_second': 2.39, 'eval_steps_per_second': 0.478, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4149, 'learning_rate': 0.002785714285714286, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.641923427581787, 'eval_runtime': 2.1386, 'eval_samples_per_second': 2.338, 'eval_steps_per_second': 0.468, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m data \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m samples:\n\u001b[0;32m----> 3\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mlearn_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mdict\u001b[39m(before\u001b[38;5;241m=\u001b[39mr[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore\u001b[39m\u001b[38;5;124m'\u001b[39m], after\u001b[38;5;241m=\u001b[39mr[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n",
      "Cell \u001b[0;32mIn[18], line 50\u001b[0m, in \u001b[0;36mlearn_sample\u001b[0;34m(sample)\u001b[0m\n\u001b[1;32m     48\u001b[0m trainer\u001b[38;5;241m.\u001b[39m_signature_columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m,]\n\u001b[1;32m     49\u001b[0m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# silence the warnings. Please re-enable for inference!\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m train_output \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m df_hist \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(trainer\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mlog_history)\n\u001b[1;32m     53\u001b[0m df_hist_epoch \u001b[38;5;241m=\u001b[39m df_hist\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mlast()\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/transformers/trainer.py:1591\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1589\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1590\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1591\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1592\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1593\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1594\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1595\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1596\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/transformers/trainer.py:1892\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1889\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   1891\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1892\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1895\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1896\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1897\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1898\u001b[0m ):\n\u001b[1;32m   1899\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1900\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/transformers/trainer.py:2787\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2785\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   2786\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2787\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2789\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[0;32m/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/accelerate/accelerator.py:1903\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   1901\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1902\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1903\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1904\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1905\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data = []\n",
    "for sample in samples:\n",
    "    r = learn_sample(sample)\n",
    "    print(sample['title'])\n",
    "    print(dict(before=r['before'], after=r['after']))\n",
    "    data.append(dict(**r, **sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# example training\n",
    "df_hist = data[-1]['hist']#.groupby('epoch').last().dropna(axis=1).drop(columns=['step'])\n",
    "df_hist['loss'].plot(label='train')\n",
    "plt.twinx()\n",
    "df_hist['eval_loss'].plot(c='b', label='eval')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='epoch'>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_hist['learning_rate'].plot(logy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Perplexity\n",
    "\n",
    "Perplexity measures how well a language model predicts a text sample. Lower is better\n",
    "\n",
    "It’s calculated as the average number of bits per word a model needs to represent the same\n",
    "\n",
    "https://huggingface.co/docs/transformers/perplexity\n",
    "https://thegradient.pub/understanding-evaluation-metrics-for-language-models/\n",
    "\n",
    "The **improvement** column, is perplexity decrease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>before</th>\n",
       "      <th>after</th>\n",
       "      <th>len</th>\n",
       "      <th>improvement%</th>\n",
       "      <th>improvement</th>\n",
       "      <th>novel</th>\n",
       "      <th>learnable</th>\n",
       "      <th>BS</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>title</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>How to Catch an AI Liar</th>\n",
       "      <td>28.927479</td>\n",
       "      <td>23.912350</td>\n",
       "      <td>5464</td>\n",
       "      <td>0.173369</td>\n",
       "      <td>5.015129</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ibois, Philippe (2012-06-03).</th>\n",
       "      <td>72.714935</td>\n",
       "      <td>63.473969</td>\n",
       "      <td>13707</td>\n",
       "      <td>0.127085</td>\n",
       "      <td>9.240967</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>buzzfeed foi fauci emails 2023</th>\n",
       "      <td>23.320093</td>\n",
       "      <td>20.782446</td>\n",
       "      <td>13640</td>\n",
       "      <td>0.108818</td>\n",
       "      <td>2.537647</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disney appointment</th>\n",
       "      <td>118.502907</td>\n",
       "      <td>110.344528</td>\n",
       "      <td>3653</td>\n",
       "      <td>0.068845</td>\n",
       "      <td>8.158379</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blechley declaration</th>\n",
       "      <td>17.862339</td>\n",
       "      <td>16.786160</td>\n",
       "      <td>7762</td>\n",
       "      <td>0.060249</td>\n",
       "      <td>1.076180</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fake ai hoax paper</th>\n",
       "      <td>7.748084</td>\n",
       "      <td>7.473690</td>\n",
       "      <td>3290</td>\n",
       "      <td>0.035414</td>\n",
       "      <td>0.274394</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>harvard announcment caplain israel hamas</th>\n",
       "      <td>45.436657</td>\n",
       "      <td>44.633602</td>\n",
       "      <td>4247</td>\n",
       "      <td>0.017674</td>\n",
       "      <td>0.803055</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              before       after    len  \\\n",
       "title                                                                     \n",
       "How to Catch an AI Liar                    28.927479   23.912350   5464   \n",
       "ibois, Philippe (2012-06-03).              72.714935   63.473969  13707   \n",
       "buzzfeed foi fauci emails 2023             23.320093   20.782446  13640   \n",
       "disney appointment                        118.502907  110.344528   3653   \n",
       "blechley declaration                       17.862339   16.786160   7762   \n",
       "fake ai hoax paper                          7.748084    7.473690   3290   \n",
       "harvard announcment caplain israel hamas   45.436657   44.633602   4247   \n",
       "\n",
       "                                          improvement%  improvement  novel  \\\n",
       "title                                                                        \n",
       "How to Catch an AI Liar                       0.173369     5.015129   True   \n",
       "ibois, Philippe (2012-06-03).                 0.127085     9.240967   True   \n",
       "buzzfeed foi fauci emails 2023                0.108818     2.537647   True   \n",
       "disney appointment                            0.068845     8.158379   True   \n",
       "blechley declaration                          0.060249     1.076180   True   \n",
       "fake ai hoax paper                            0.035414     0.274394  False   \n",
       "harvard announcment caplain israel hamas      0.017674     0.803055   True   \n",
       "\n",
       "                                          learnable     BS  \n",
       "title                                                       \n",
       "How to Catch an AI Liar                        True  False  \n",
       "ibois, Philippe (2012-06-03).                  True  False  \n",
       "buzzfeed foi fauci emails 2023                 True  False  \n",
       "disney appointment                             True  False  \n",
       "blechley declaration                           True  False  \n",
       "fake ai hoax paper                             True   True  \n",
       "harvard announcment caplain israel hamas      False   True  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_res = pd.DataFrame(data)\n",
    "df_res['len'] = df_res.content.str.len()\n",
    "df_res = df_res[['before', 'after', 'title',  'len']].set_index('title')\n",
    "df_res['improvement%'] = (df_res['before'] - df_res['after'])/ df_res['before']\n",
    "df_res['improvement'] = (df_res['before'] - df_res['after'])\n",
    "df_res['novel'] = df_res['before'] > 15\n",
    "df_res['learnable'] = df_res['improvement%'] > 0.02\n",
    "\n",
    "# We can measure the final score using learnable * novel\n",
    "# df_res['BS'] = ~df_res['learnable'] | ~df_res['novel']\n",
    "# Or just absolute perplexity improvement\n",
    "df_res['BS'] = df_res['improvement'] < 1\n",
    "df_res = df_res.sort_values('improvement', ascending=False)\n",
    "df_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| title                                    |    before |     after |   len |   improvement% |   improvement | novel   | learnable   | BS    |\n",
      "|:-----------------------------------------|----------:|----------:|------:|---------------:|--------------:|:--------|:------------|:------|\n",
      "| How to Catch an AI Liar                  |  28.9275  |  23.9123  |  5464 |      0.173369  |      5.01513  | True    | True        | False |\n",
      "| ibois, Philippe (2012-06-03).            |  72.7149  |  63.474   | 13707 |      0.127085  |      9.24097  | True    | True        | False |\n",
      "| buzzfeed foi fauci emails 2023           |  23.3201  |  20.7824  | 13640 |      0.108818  |      2.53765  | True    | True        | False |\n",
      "| disney appointment                       | 118.503   | 110.345   |  3653 |      0.0688454 |      8.15838  | True    | True        | False |\n",
      "| blechley declaration                     |  17.8623  |  16.7862  |  7762 |      0.0602485 |      1.07618  | True    | True        | False |\n",
      "| fake ai hoax paper                       |   7.74808 |   7.47369 |  3290 |      0.0354144 |      0.274394 | False   | True        | True  |\n",
      "| harvard announcment caplain israel hamas |  45.4367  |  44.6336  |  4247 |      0.0176742 |      0.803055 | True    | False       | True  |\n"
     ]
    }
   ],
   "source": [
    "print(df_res.to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEBUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML, Markdown\n",
    "import torch\n",
    "\n",
    "@torch.no_grad()\n",
    "def gen(model, inputs, tokenizer, clean=True):\n",
    "    s = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"][None, :].to(model.device),\n",
    "        attention_mask=inputs[\"attention_mask\"][None, :].to(model.device),\n",
    "        use_cache=False,\n",
    "        max_new_tokens=100,\n",
    "        min_new_tokens=100,\n",
    "        do_sample=False,\n",
    "        early_stopping=False,\n",
    "    )\n",
    "    input_l = inputs[\"input_ids\"].shape[0]\n",
    "    tokenizer_kwargs=dict(clean_up_tokenization_spaces=clean, skip_special_tokens=clean)\n",
    "    old = tokenizer.decode(\n",
    "        s[0, :input_l][-100:], **tokenizer_kwargs\n",
    "    )\n",
    "    new = tokenizer.decode(\n",
    "        s[0, input_l:], **tokenizer_kwargs\n",
    "    )\n",
    "    s_old = \"\"+old.replace('\\n', '<br>')\n",
    "    s_new =  '<b>' + new.replace('\\n', '<br>')+ '<br><br><b/>'\n",
    "    # print(s_old, s_new)\n",
    "    display(HTML(f\"{s_old}{s_new}\"))\n",
    "    # print([old, new])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = samples[-1]\n",
    "s = sample['content']\n",
    "first_half = s[:len(s)//2]\n",
    "second_half = s[len(s)//2:]\n",
    "ds_train = Dataset.from_dict(tokenizer([first_half]))\n",
    "ds_val = Dataset.from_dict(tokenizer([second_half]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 95, 96, 97]. This is achieved through model parallelism across specialized experts, allowing the training of models with trillions of parameters, and its specialization in handling diverse data distributions enhances its capability in few-shot learning and other complex tasks [94, 95]. To illustrate the practicality of MoE, consider its application in healthcare. For example, an MoE-based system could be used for personalized medicine, where different ‘expert’ modules specialize in various aspects of patient data analysis <b>.<br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><b/>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " 95, 96, 97]. This is achieved through model parallelism across specialized experts, allowing the training of models with trillions of parameters, and its specialization in handling diverse data distributions enhances its capability in few-shot learning and other complex tasks [94, 95]. To illustrate the practicality of MoE, consider its application in healthcare. For example, an MoE-based system could be used for personalized medicine, where different ‘expert’ modules specialize in various aspects of patient data analysis<b>.<br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><b/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with model.disable_adapter():\n",
    "    gen(model, ds_train.with_format('pt')[0], tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       " 95, 96, 97]. This is achieved through model parallelism across specialized experts, allowing the training of models with trillions of parameters, and its specialization in handling diverse data distributions enhances its capability in few-shot learning and other complex tasks [94, 95]. To illustrate the practicality of MoE, consider its application in healthcare. For example, an MoE-based system could be used for personalized medicine, where different ‘expert’ modules specialize in various aspects of patient data analysis<b>.<br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><b/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gen(model, ds_train.with_format('pt')[0], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
