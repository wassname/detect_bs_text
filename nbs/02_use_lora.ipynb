{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/huggingface/peft/blob/main/examples/fp4_finetuning/finetune_fp4_opt_bnb_peft.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "import lightning as pl\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoConfig\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from peft import LoraConfig, get_peft_model, IA3Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "warnings.filterwarnings(\"ignore\", \".*does not have many workers.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "model_name = \"microsoft/phi-2\"\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_name,\n",
    "#     # max_memory=max_memory,\n",
    "#     quantization_config=BitsAndBytesConfig(\n",
    "#         load_in_4bit=True,\n",
    "#         llm_int8_threshold=6.0,\n",
    "#         llm_int8_has_fp16_weight=False,\n",
    "#         bnb_4bit_compute_dtype=torch.float16,\n",
    "#         bnb_4bit_use_double_quant=True,\n",
    "#         bnb_4bit_quant_type=\"nf4\",\n",
    "#     ),\n",
    "#     torch_dtype=torch.float16,\n",
    "#     trust_remote_code=True,\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"TheBloke/phi-2-GPTQ\"\n",
    "# model_name = \"microsoft/phi-2\"\n",
    "\n",
    "def load_model():\n",
    "\n",
    "    # model = AutoModelForCausalLM.from_pretrained(\n",
    "    #     model_name,\n",
    "    #     # quantization_config=BitsAndBytesConfig(\n",
    "    #     #     load_in_4bit=True,\n",
    "    #     #     llm_int8_threshold=6.0,\n",
    "    #     #     llm_int8_has_fp16_weight=False,\n",
    "    #     #     bnb_4bit_compute_dtype=torch.float16,\n",
    "    #     #     bnb_4bit_use_double_quant=True,\n",
    "    #     #     bnb_4bit_quant_type=\"nf4\",\n",
    "    #     # ),\n",
    "    #     torch_dtype=torch.float16,\n",
    "    #     trust_remote_code=True,\n",
    "    # )\n",
    "\n",
    "\n",
    "    config = AutoConfig.from_pretrained(model_name, trust_remote_code=True,)\n",
    "    config.quantization_config['use_exllama'] = False\n",
    "    # del config.quantization_config['use_exllama']\n",
    "    config.quantization_config['disable_exllama'] = True\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        trust_remote_code=True,\n",
    "        config=config,\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "MAX_LEN = 2000\n",
    "samples = json.load(open(\"../samples.json\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modified from https://github.dev/huggingface/evaluate/blob/8dfe05784099fb9af55b8e77793205a3b7c86465/measurements/perplexity/perplexity.py#L154\n",
    "\n",
    "# from evaluate.measurements.perplexity import Perplexity\n",
    "import evaluate\n",
    "from evaluate import logging\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "# @evaluate.utils.file_utils.add_start_docstrings(_DESCRIPTION, _KWARGS_DESCRIPTION)\n",
    "def perplexity_compute(\n",
    "    data, model, tokenizer, batch_size: int = 16, add_start_token: bool = True, device=None, max_length=None\n",
    "):\n",
    "\n",
    "    if device is not None:\n",
    "        assert device in [\"gpu\", \"cpu\", \"cuda\"], \"device should be either gpu or cpu.\"\n",
    "        if device == \"gpu\":\n",
    "            device = \"cuda\"\n",
    "    else:\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "    model = model.to(device)\n",
    "\n",
    "    # tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "    # if batch_size > 1 (which generally leads to padding being required), and\n",
    "    # if there is not an already assigned pad_token, assign an existing\n",
    "    # special token to also be the padding token\n",
    "    if tokenizer.pad_token is None and batch_size > 1:\n",
    "        existing_special_tokens = list(tokenizer.special_tokens_map_extended.values())\n",
    "        # check that the model already has at least one special token defined\n",
    "        assert (\n",
    "            len(existing_special_tokens) > 0\n",
    "        ), \"If batch_size > 1, model must have at least one special token to use for padding. Please use a different model or set batch_size=1.\"\n",
    "        # assign one of the special tokens to also be the pad token\n",
    "        tokenizer.add_special_tokens({\"pad_token\": existing_special_tokens[0]})\n",
    "\n",
    "    if add_start_token and max_length:\n",
    "        # leave room for <BOS> token to be added:\n",
    "        assert (\n",
    "            tokenizer.bos_token is not None\n",
    "        ), \"Input model must already have a BOS token if using add_start_token=True. Please use a different model, or set add_start_token=False\"\n",
    "        max_tokenized_len = max_length - 1\n",
    "    else:\n",
    "        max_tokenized_len = max_length\n",
    "\n",
    "    encodings = tokenizer(\n",
    "        data,\n",
    "        add_special_tokens=False,\n",
    "        padding=True,\n",
    "        truncation=True if max_tokenized_len else False,\n",
    "        max_length=max_tokenized_len,\n",
    "        return_tensors=\"pt\",\n",
    "        return_attention_mask=True,\n",
    "    ).to(device)\n",
    "\n",
    "    encoded_texts = encodings[\"input_ids\"]\n",
    "    attn_masks = encodings[\"attention_mask\"]\n",
    "\n",
    "    # check that each input is long enough:\n",
    "    if add_start_token:\n",
    "        assert torch.all(torch.ge(attn_masks.sum(1), 1)), \"Each input text must be at least one token long.\"\n",
    "    else:\n",
    "        assert torch.all(\n",
    "            torch.ge(attn_masks.sum(1), 2)\n",
    "        ), \"When add_start_token=False, each input text must be at least two tokens long. Run with add_start_token=True if inputting strings of only one token, and remove all empty input strings.\"\n",
    "\n",
    "    ppls = []\n",
    "    loss_fct = CrossEntropyLoss(reduction=\"none\")\n",
    "\n",
    "    for start_index in logging.tqdm(range(0, len(encoded_texts), batch_size)):\n",
    "        end_index = min(start_index + batch_size, len(encoded_texts))\n",
    "        encoded_batch = encoded_texts[start_index:end_index]\n",
    "        attn_mask = attn_masks[start_index:end_index]\n",
    "\n",
    "        if add_start_token:\n",
    "            bos_tokens_tensor = torch.tensor([[tokenizer.bos_token_id]] * encoded_batch.size(dim=0)).to(device)\n",
    "            encoded_batch = torch.cat([bos_tokens_tensor, encoded_batch], dim=1)\n",
    "            attn_mask = torch.cat(\n",
    "                [torch.ones(bos_tokens_tensor.size(), dtype=torch.int64).to(device), attn_mask], dim=1\n",
    "            )\n",
    "\n",
    "        labels = encoded_batch\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out_logits = model(encoded_batch, attention_mask=attn_mask).logits\n",
    "\n",
    "        shift_logits = out_logits[..., :-1, :].contiguous()\n",
    "        shift_labels = labels[..., 1:].contiguous()\n",
    "        shift_attention_mask_batch = attn_mask[..., 1:].contiguous()\n",
    "\n",
    "        perplexity_batch = torch.exp(\n",
    "            (loss_fct(shift_logits.transpose(1, 2), shift_labels) * shift_attention_mask_batch).sum(1)\n",
    "            / shift_attention_mask_batch.sum(1)\n",
    "        )\n",
    "\n",
    "        ppls += perplexity_batch.tolist()\n",
    "\n",
    "    return {\"perplexities\": ppls, \"mean_perplexity\": torch.tensor(ppls).mean()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lightning helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = samples[0]\n",
    "s = sample['text']\n",
    "first_half = s[:len(s)//2]\n",
    "second_half = s[len(s)//2:]\n",
    "\n",
    "\n",
    "\n",
    "def str2xya(s, tokenizer):\n",
    "    max_len = min(MAX_LEN, len(s))\n",
    "    input_ids = tokenizer(s, return_tensors=\"pt\")[\"input_ids\"][0].tolist()\n",
    "\n",
    "    pad = tokenizer.bos_token_id\n",
    "    # turn it into a sequence\n",
    "    Xs = []\n",
    "    Ys = []\n",
    "    for i in range(1, len(input_ids)):\n",
    "        x = input_ids[:i][-max_len:]\n",
    "        padding = max_len - len(x)\n",
    "        x = [pad]*padding + x\n",
    "        \n",
    "        Xs.append(x)\n",
    "        Ys.append(input_ids[i:i+1])\n",
    "\n",
    "    Xs = torch.tensor(Xs)\n",
    "    Ys = torch.tensor(Ys)\n",
    "    attention_masks = torch.stack([(x==pad)*1 for x in Xs])\n",
    "    return Xs, Ys, attention_masks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, tokenizer, second_half):\n",
    "    model.eval();\n",
    "    with torch.no_grad():\n",
    "        with model.disable_adapter():\n",
    "            results = perplexity_compute(data=second_half, model=model, tokenizer=tokenizer, device='cuda')\n",
    "        results2 = perplexity_compute(data=second_half, model=model, tokenizer=tokenizer, device='cuda')\n",
    "    return dict(before=results['mean_perplexity'].item(), after=results2['mean_perplexity'].item())\n",
    "\n",
    "def read_metrics_csv(metrics_file_path):\n",
    "    df_hist = pd.read_csv(metrics_file_path)\n",
    "    df_hist[\"epoch\"] = df_hist[\"epoch\"].ffill()\n",
    "    df_histe = df_hist.set_index(\"epoch\").groupby(\"epoch\").mean()\n",
    "    return df_histe, df_hist\n",
    "\n",
    "\n",
    "def plot_hist(df_hist, allowlist=None, logy=False):\n",
    "    \"\"\"plot groups of suffixes together\"\"\"\n",
    "    suffixes = list(set([c.split('/')[-1] for c in df_hist.columns if '/' in c]))\n",
    "    for suffix in suffixes:\n",
    "        if allowlist and suffix not in allowlist: continue\n",
    "        df_hist[[c for c in df_hist.columns if c.endswith(suffix) and '/' in c]].plot(title=suffix, style='.', logy=logy)\n",
    "        plt.title(suffix)   \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class PL_MODEL(pl.LightningModule):\n",
    "    def __init__(self, num_iterations, lr=3e-4, weight_decay=0,):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def configure_model(self):\n",
    "        # instantiate your model in this hook\n",
    "        peft_config = LoraConfig(\n",
    "            # task_type=TaskType.TOKEN_CLS, \n",
    "            target_modules=[ \"fc2\",  \"Wqkv\",],\n",
    "            inference_mode=False, r=16, lora_alpha=16, \n",
    "            # lora_dropout=0.1,\n",
    "            # bias=\"all\"\n",
    "        )\n",
    "        self.model = load_model()\n",
    "        self.model = get_peft_model(self.model, peft_config)\n",
    "        self.model.config.use_cache = False\n",
    "    \n",
    "    def forward(self, **kwargs):\n",
    "        return self.model(**kwargs)\n",
    "\n",
    "    def _shared_step(self, batch, batch_idx, phase='train'):\n",
    "        input_ids, targets, attention_mask = batch\n",
    "        # 16, 141\n",
    "        output = self.forward(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = F.smooth_l1_loss(output.logits[:, -1], targets)\n",
    "        self.log(f\"{phase}/loss\", loss, on_epoch=True, on_step=True, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self._shared_step(batch, batch_idx, phase='train')\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self._shared_step(batch, batch_idx, phase='val')\n",
    "    \n",
    "    def test_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        return self._shared_step(batch, batch_idx, phase='test')\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(self.parameters(), lr=self.hparams.lr, weight_decay=self.hparams.weight_decay)\n",
    "        lr_scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer, self.hparams.lr, total_steps=self.hparams.num_iterations\n",
    "        )\n",
    "        return [optimizer], [lr_scheduler]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'PL_MODEL' object has no attribute 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m epoch_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(dl_train)\n\u001b[1;32m     14\u001b[0m pl_model \u001b[38;5;241m=\u001b[39m PL_MODEL(num_iterations\u001b[38;5;241m=\u001b[39mepoch_steps\u001b[38;5;241m*\u001b[39mepochs, lr\u001b[38;5;241m=\u001b[39mlr, weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mpl_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# from lightning.pytorch.plugins import BitsandbytesPrecision\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# precision = BitsandbytesPrecision(mode=\"nf4-dq\")\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# precision = BitsandbytesPrecision(mode=\"int8-training\", dtype=torch.float16, ignore_modules={\"lm_head\"})\u001b[39;00m\n\u001b[1;32m     19\u001b[0m trainer \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mTrainer(\n\u001b[1;32m     20\u001b[0m         max_epochs\u001b[38;5;241m=\u001b[39mepochs,\n\u001b[1;32m     21\u001b[0m         \u001b[38;5;66;03m# precision=\"bf16-mixed\",\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[38;5;66;03m# plugins=precision\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     )\n",
      "File \u001b[0;32m/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1695\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1693\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1694\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1695\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'PL_MODEL' object has no attribute 'model'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "device = 'cuda'\n",
    "lr = 4e-3\n",
    "epochs = 3\n",
    "accum_steps = 16\n",
    "batch_size = 1\n",
    "\n",
    "Xs, Ys, attention_masks = str2xya(first_half, tokenizer)\n",
    "dl_train = DataLoader(TensorDataset(Xs, Ys, attention_masks), batch_size=batch_size, shuffle=True)\n",
    "Xs, Ys, attention_masks = str2xya(second_half, tokenizer)\n",
    "dl_val = DataLoader(TensorDataset(Xs, Ys, attention_masks), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "epoch_steps = len(dl_train)\n",
    "\n",
    "pl_model = PL_MODEL(num_iterations=epoch_steps*epochs, lr=lr, weight_decay=0)\n",
    "model = pl_model.model\n",
    "# from lightning.pytorch.plugins import BitsandbytesPrecision\n",
    "# precision = BitsandbytesPrecision(mode=\"nf4-dq\")\n",
    "# precision = BitsandbytesPrecision(mode=\"int8-training\", dtype=torch.float16, ignore_modules={\"lm_head\"})\n",
    "trainer = pl.Trainer(\n",
    "        accelerator='cpu',\n",
    "        max_epochs=epochs,\n",
    "        precision='',\n",
    "        # precision=\"bf16-mixed\",\n",
    "        log_every_n_steps=1,\n",
    "        accumulate_grad_batches=accum_steps,\n",
    "        # plugins=precision\n",
    "    )\n",
    "\n",
    "# train\n",
    "trainer.fit(pl_model, dl_train, dl_val)\n",
    "\n",
    "\n",
    "df_histe, df_hist = read_metrics_csv(trainer.logger.experiment.metrics_file_path).bfill().ffill()\n",
    "display(df_hist)\n",
    "plot_hist(df_hist)\n",
    "\n",
    "eval(model, tokenizer, second_half)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "\n",
    "def lora_eval(model, tokenizer, sample):\n",
    "    # reset/set adapter\n",
    "    # peft_config = IA3Config(\n",
    "    #     target_modules=[ \"fc2\",  \"Wqkv\",], \n",
    "    #         feedforward_modules=[\"fc2\"],\n",
    "    #         inference_mode=False,\n",
    "    # )\n",
    "    peft_config = LoraConfig(\n",
    "        # task_type=TaskType.TOKEN_CLS, \n",
    "        target_modules=[ \"fc2\",  \"Wqkv\",],\n",
    "        inference_mode=False, r=16, lora_alpha=16, lora_dropout=0.1, bias=\"all\"\n",
    "    )\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    model.config.use_cache = False\n",
    "\n",
    "    # train adapter\n",
    "    s = sample['text']\n",
    "    first_half = s[:len(s)//2]\n",
    "    second_half = s[len(s)//2:]\n",
    "    input_ids = tokenizer(first_half, return_tensors=\"pt\")[\"input_ids\"][0].to('cuda')\n",
    "\n",
    "    device = 'cuda'\n",
    "    lr = 1.0e-2\n",
    "    epochs = 3\n",
    "    accum_steps = 64\n",
    "    epoch_steps = (len(input_ids)-1)//accum_steps+1\n",
    "\n",
    "    total_steps = epochs * epoch_steps\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer, lr, total_steps=total_steps\n",
    "    )\n",
    "    model.train()\n",
    "    model = model.to(device)\n",
    "    for epoch in range(epochs):\n",
    "        # TODO: batch\n",
    "        \n",
    "        accum = 0\n",
    "        for i in range(1, len(input_ids)):\n",
    "            X = input_ids[:i][None, ]\n",
    "            targets = input_ids[i:i+1][None, ]\n",
    "            optimizer.zero_grad()\n",
    "            out = model(input_ids=X, \n",
    "                        )\n",
    "            logits = out['logits'][:, -1]\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "            loss.backward()\n",
    "            if accum > accum_steps:\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                accum = 0\n",
    "            else:\n",
    "                accum += 1\n",
    "        if accum > 0:\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "    return eval(model, tokenizer, second_half)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for sample in tqdm(samples):\n",
    "    r = lora_eval(model, tokenizer, sample)\n",
    "    print(sample['name'], r)\n",
    "    r.update(sample)\n",
    "    data.append(r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('perplexity (on 2nd half) before and after training adapter on first half of text')\n",
    "df = pd.DataFrame(data).set_index('name')\n",
    "\n",
    "df['learning'] = (df['before']-df['after'])/df['before']\n",
    "df.sort_values('learning').drop(columns=['text', 'url'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
