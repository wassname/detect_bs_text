{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/huggingface/peft/blob/main/examples/fp4_finetuning/finetune_fp4_opt_bnb_peft.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "from peft import LoraConfig, get_peft_model, IA3Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.34it/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "model_name = \"microsoft/phi-2\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    # max_memory=max_memory,\n",
    "    # quantization_config=BitsAndBytesConfig(\n",
    "    #     load_in_4bit=True,\n",
    "    #     llm_int8_threshold=6.0,\n",
    "    #     llm_int8_has_fp16_weight=False,\n",
    "    #     bnb_4bit_compute_dtype=torch.float16,\n",
    "    #     bnb_4bit_use_double_quant=True,\n",
    "    #     bnb_4bit_quant_type=\"nf4\",\n",
    "    # ),\n",
    "    # torch_dtype=torch.float16,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True,)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 2000\n",
    "import json\n",
    "samples = json.load(open(\"../samples.json\"))\n",
    "\n",
    "# sample = samples[0]\n",
    "# sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modified from https://github.dev/huggingface/evaluate/blob/8dfe05784099fb9af55b8e77793205a3b7c86465/measurements/perplexity/perplexity.py#L154\n",
    "\n",
    "# from evaluate.measurements.perplexity import Perplexity\n",
    "import evaluate\n",
    "from evaluate import logging\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "# @evaluate.utils.file_utils.add_start_docstrings(_DESCRIPTION, _KWARGS_DESCRIPTION)\n",
    "def perplexity_compute(\n",
    "    data, model, tokenizer, batch_size: int = 16, add_start_token: bool = True, device=None, max_length=None\n",
    "):\n",
    "\n",
    "    if device is not None:\n",
    "        assert device in [\"gpu\", \"cpu\", \"cuda\"], \"device should be either gpu or cpu.\"\n",
    "        if device == \"gpu\":\n",
    "            device = \"cuda\"\n",
    "    else:\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "    model = model.to(device)\n",
    "\n",
    "    # tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "    # if batch_size > 1 (which generally leads to padding being required), and\n",
    "    # if there is not an already assigned pad_token, assign an existing\n",
    "    # special token to also be the padding token\n",
    "    if tokenizer.pad_token is None and batch_size > 1:\n",
    "        existing_special_tokens = list(tokenizer.special_tokens_map_extended.values())\n",
    "        # check that the model already has at least one special token defined\n",
    "        assert (\n",
    "            len(existing_special_tokens) > 0\n",
    "        ), \"If batch_size > 1, model must have at least one special token to use for padding. Please use a different model or set batch_size=1.\"\n",
    "        # assign one of the special tokens to also be the pad token\n",
    "        tokenizer.add_special_tokens({\"pad_token\": existing_special_tokens[0]})\n",
    "\n",
    "    if add_start_token and max_length:\n",
    "        # leave room for <BOS> token to be added:\n",
    "        assert (\n",
    "            tokenizer.bos_token is not None\n",
    "        ), \"Input model must already have a BOS token if using add_start_token=True. Please use a different model, or set add_start_token=False\"\n",
    "        max_tokenized_len = max_length - 1\n",
    "    else:\n",
    "        max_tokenized_len = max_length\n",
    "\n",
    "    encodings = tokenizer(\n",
    "        data,\n",
    "        add_special_tokens=False,\n",
    "        padding=True,\n",
    "        truncation=True if max_tokenized_len else False,\n",
    "        max_length=max_tokenized_len,\n",
    "        return_tensors=\"pt\",\n",
    "        return_attention_mask=True,\n",
    "    ).to(device)\n",
    "\n",
    "    encoded_texts = encodings[\"input_ids\"]\n",
    "    attn_masks = encodings[\"attention_mask\"]\n",
    "\n",
    "    # check that each input is long enough:\n",
    "    if add_start_token:\n",
    "        assert torch.all(torch.ge(attn_masks.sum(1), 1)), \"Each input text must be at least one token long.\"\n",
    "    else:\n",
    "        assert torch.all(\n",
    "            torch.ge(attn_masks.sum(1), 2)\n",
    "        ), \"When add_start_token=False, each input text must be at least two tokens long. Run with add_start_token=True if inputting strings of only one token, and remove all empty input strings.\"\n",
    "\n",
    "    ppls = []\n",
    "    loss_fct = CrossEntropyLoss(reduction=\"none\")\n",
    "\n",
    "    for start_index in logging.tqdm(range(0, len(encoded_texts), batch_size)):\n",
    "        end_index = min(start_index + batch_size, len(encoded_texts))\n",
    "        encoded_batch = encoded_texts[start_index:end_index]\n",
    "        attn_mask = attn_masks[start_index:end_index]\n",
    "\n",
    "        if add_start_token:\n",
    "            bos_tokens_tensor = torch.tensor([[tokenizer.bos_token_id]] * encoded_batch.size(dim=0)).to(device)\n",
    "            encoded_batch = torch.cat([bos_tokens_tensor, encoded_batch], dim=1)\n",
    "            attn_mask = torch.cat(\n",
    "                [torch.ones(bos_tokens_tensor.size(), dtype=torch.int64).to(device), attn_mask], dim=1\n",
    "            )\n",
    "\n",
    "        labels = encoded_batch\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out_logits = model(encoded_batch, attention_mask=attn_mask).logits\n",
    "\n",
    "        shift_logits = out_logits[..., :-1, :].contiguous()\n",
    "        shift_labels = labels[..., 1:].contiguous()\n",
    "        shift_attention_mask_batch = attn_mask[..., 1:].contiguous()\n",
    "\n",
    "        perplexity_batch = torch.exp(\n",
    "            (loss_fct(shift_logits.transpose(1, 2), shift_labels) * shift_attention_mask_batch).sum(1)\n",
    "            / shift_attention_mask_batch.sum(1)\n",
    "        )\n",
    "\n",
    "        ppls += perplexity_batch.tolist()\n",
    "\n",
    "    return {\"perplexities\": ppls, \"mean_perplexity\": torch.tensor(ppls).mean()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = perplexity_compute(data=sample['text'], model=model, tokenizer=tokenizer, device='cuda')\n",
    "# results['mean_perplexity']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"### Post-processing on the model\n",
    "\n",
    "# Finally, we need to apply some post-processing on the 8-bit model to enable training, let's freeze all our layers, and cast the layer-norm in `float32` for stability. We also cast the output of the last layer in `float32` for the same reasons.\n",
    "# \"\"\"\n",
    "\n",
    "# print(model)\n",
    "\n",
    "# for param in model.parameters():\n",
    "#     param.requires_grad = False  # freeze the model - train adapters later\n",
    "#     if param.ndim == 1:\n",
    "#         # cast the small parameters (e.g. layernorm) to fp32 for stability\n",
    "#         param.data = param.data.to(torch.float32)\n",
    "\n",
    "# # model.gradient_checkpointing_enable()  # reduce number of stored activations\n",
    "# # model.model.decoder.project_in = lambda x: x.requires_grad_(True)\n",
    "\n",
    "\n",
    "# class CastOutputToFloat(nn.Sequential):\n",
    "#     def forward(self, x):\n",
    "#         return super().forward(x).to(torch.float32)\n",
    "\n",
    "\n",
    "# model.lm_head = CastOutputToFloat(model.lm_head)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "\n",
    "def lora_eval(model, tokenizer, sample):\n",
    "    # reset/set adapter\n",
    "    # peft_config = IA3Config(\n",
    "    #     target_modules=[ \"fc2\",  \"Wqkv\",], \n",
    "    #         feedforward_modules=[\"fc2\"],\n",
    "    #         inference_mode=False,\n",
    "    # )\n",
    "    peft_config = LoraConfig(\n",
    "        # task_type=TaskType.TOKEN_CLS, \n",
    "        target_modules=[ \"fc2\",  \"Wqkv\",],\n",
    "        inference_mode=False, r=16, lora_alpha=16, lora_dropout=0.1, bias=\"all\"\n",
    "    )\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    model.config.use_cache = False\n",
    "\n",
    "    # train adapter\n",
    "    s = sample['text']\n",
    "    first_half = s[:len(s)//2]\n",
    "    second_half = s[len(s)//2:]\n",
    "    input_ids = tokenizer(first_half, return_tensors=\"pt\")[\"input_ids\"][0].to('cuda')\n",
    "\n",
    "    device = 'cuda'\n",
    "    lr = 1.0e-2\n",
    "    epochs = 3\n",
    "    accum_steps = 64\n",
    "    epoch_steps = (len(input_ids)-1)//accum_steps+1\n",
    "\n",
    "    total_steps = epochs * epoch_steps\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer, lr, total_steps=total_steps\n",
    "    )\n",
    "    model.train()\n",
    "    model = model.to(device)\n",
    "    for epoch in range(epochs):\n",
    "        # TODO: batch\n",
    "        \n",
    "        accum = 0\n",
    "        for i in range(1, len(input_ids)):\n",
    "            X = input_ids[:i][None, ]\n",
    "            targets = input_ids[i:i+1][None, ]\n",
    "            optimizer.zero_grad()\n",
    "            out = model(input_ids=X, \n",
    "                        )\n",
    "            logits = out['logits'][:, -1]\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "            loss.backward()\n",
    "            if accum > accum_steps:\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                accum = 0\n",
    "            else:\n",
    "                accum += 1\n",
    "        if accum > 0:\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "    # eval\n",
    "    model.eval();\n",
    "    with torch.no_grad():\n",
    "        with model.disable_adapter():\n",
    "            results = perplexity_compute(data=second_half, model=model, tokenizer=tokenizer, device='cuda')\n",
    "            results['mean_perplexity']\n",
    "        results2 = perplexity_compute(data=second_half, model=model, tokenizer=tokenizer, device='cuda')\n",
    "\n",
    "    return dict(before=results['mean_perplexity'].item(), after=results2['mean_perplexity'].item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/12 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/bs_writing_detector/.venv/lib/python3.11/site-packages/peft/tuners/lora/model.py:402: UserWarning: Careful, disabling adapter layers with bias configured to be 'all' does not produce the same output as the the base model would without adaption.\n",
      "  warnings.warn(msg)\n",
      "100%|██████████| 1/1 [00:00<00:00,  6.42it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  6.36it/s]\n",
      "  8%|▊         | 1/12 [01:21<14:54, 81.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bad_ml {'before': 17.1319522857666, 'after': 17.076616287231445}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.21it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  9.19it/s]\n",
      " 17%|█▋        | 2/12 [01:43<07:45, 46.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good_ml {'before': 48.654518127441406, 'after': 48.63978576660156}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  6.00it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  6.20it/s]\n",
      " 25%|██▌       | 3/12 [03:26<10:49, 72.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sokal hoax {'before': 29.55867576599121, 'after': 29.561065673828125}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  7.25it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  7.55it/s]\n",
      " 33%|███▎      | 4/12 [04:10<08:10, 61.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theory o. general relativity {'before': 48.4825553894043, 'after': 48.46034622192383}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  8.83it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  9.19it/s]\n",
      " 42%|████▏     | 5/12 [04:29<05:21, 45.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lorem ipsum  {'before': 243.0447540283203, 'after': 238.47674560546875}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  8.78it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.88it/s]\n",
      " 50%|█████     | 6/12 [05:05<04:14, 42.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wikipedia on LK-99 {'before': 53.24197006225586, 'after': 53.270263671875}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  8.77it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.92it/s]\n",
      " 58%|█████▊    | 7/12 [05:30<03:03, 36.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have a dream {'before': 18.867136001586914, 'after': 18.801422119140625}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  6.27it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  6.20it/s]\n",
      " 67%|██████▋   | 8/12 [06:40<03:09, 47.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI gen fake paper {'before': 11.114971160888672, 'after': 11.109580039978027}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  6.13it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  6.07it/s]\n",
      " 75%|███████▌  | 9/12 [08:43<03:32, 70.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schmidhuber 2023 Subjective Novelty, Surprise {'before': 67.33682250976562, 'after': 67.20210266113281}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  6.77it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  6.85it/s]\n",
      " 83%|████████▎ | 10/12 [09:50<02:19, 69.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "email_to_fauci {'before': 55.9570198059082, 'after': 56.01524353027344}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 27.48it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 24.11it/s]\n",
      " 92%|█████████▏| 11/12 [10:02<00:52, 52.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enron_email1 {'before': 59.76203536987305, 'after': 59.75802230834961}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  8.55it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.91it/s]\n",
      "100%|██████████| 12/12 [10:48<00:00, 54.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openai_board_ann {'before': 30.923919677734375, 'after': 30.946474075317383}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "for sample in tqdm(samples):\n",
    "    r = lora_eval(model, tokenizer, sample)\n",
    "    print(sample['name'], r)\n",
    "    r.update(sample)\n",
    "    data.append(r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity (on 2nd half) before and after training adapter on first half of text\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>before</th>\n",
       "      <th>after</th>\n",
       "      <th>in_training</th>\n",
       "      <th>learning</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>email_to_fauci</th>\n",
       "      <td>55.957020</td>\n",
       "      <td>56.015244</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.001041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>openai_board_ann</th>\n",
       "      <td>30.923920</td>\n",
       "      <td>30.946474</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.000729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wikipedia on LK-99</th>\n",
       "      <td>53.241970</td>\n",
       "      <td>53.270264</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.000531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sokal hoax</th>\n",
       "      <td>29.558676</td>\n",
       "      <td>29.561066</td>\n",
       "      <td>True</td>\n",
       "      <td>-0.000081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>enron_email1</th>\n",
       "      <td>59.762035</td>\n",
       "      <td>59.758022</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>good_ml</th>\n",
       "      <td>48.654518</td>\n",
       "      <td>48.639786</td>\n",
       "      <td>False</td>\n",
       "      <td>0.000303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Theory o. general relativity</th>\n",
       "      <td>48.482555</td>\n",
       "      <td>48.460346</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AI gen fake paper</th>\n",
       "      <td>11.114971</td>\n",
       "      <td>11.109580</td>\n",
       "      <td>False</td>\n",
       "      <td>0.000485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Schmidhuber 2023 Subjective Novelty, Surprise</th>\n",
       "      <td>67.336823</td>\n",
       "      <td>67.202103</td>\n",
       "      <td>False</td>\n",
       "      <td>0.002001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bad_ml</th>\n",
       "      <td>17.131952</td>\n",
       "      <td>17.076616</td>\n",
       "      <td>False</td>\n",
       "      <td>0.003230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I have a dream</th>\n",
       "      <td>18.867136</td>\n",
       "      <td>18.801422</td>\n",
       "      <td>True</td>\n",
       "      <td>0.003483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lorem ipsum</th>\n",
       "      <td>243.044754</td>\n",
       "      <td>238.476746</td>\n",
       "      <td>True</td>\n",
       "      <td>0.018795</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   before       after  \\\n",
       "name                                                                    \n",
       "email_to_fauci                                  55.957020   56.015244   \n",
       "openai_board_ann                                30.923920   30.946474   \n",
       "wikipedia on LK-99                              53.241970   53.270264   \n",
       "sokal hoax                                      29.558676   29.561066   \n",
       "enron_email1                                    59.762035   59.758022   \n",
       "good_ml                                         48.654518   48.639786   \n",
       "Theory o. general relativity                    48.482555   48.460346   \n",
       "AI gen fake paper                               11.114971   11.109580   \n",
       "Schmidhuber 2023 Subjective Novelty, Surprise   67.336823   67.202103   \n",
       "bad_ml                                          17.131952   17.076616   \n",
       "I have a dream                                  18.867136   18.801422   \n",
       "lorem ipsum                                    243.044754  238.476746   \n",
       "\n",
       "                                               in_training  learning  \n",
       "name                                                                  \n",
       "email_to_fauci                                       False -0.001041  \n",
       "openai_board_ann                                     False -0.000729  \n",
       "wikipedia on LK-99                                   False -0.000531  \n",
       "sokal hoax                                            True -0.000081  \n",
       "enron_email1                                          True  0.000067  \n",
       "good_ml                                              False  0.000303  \n",
       "Theory o. general relativity                          True  0.000458  \n",
       "AI gen fake paper                                    False  0.000485  \n",
       "Schmidhuber 2023 Subjective Novelty, Surprise        False  0.002001  \n",
       "bad_ml                                               False  0.003230  \n",
       "I have a dream                                        True  0.003483  \n",
       "lorem ipsum                                           True  0.018795  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('perplexity (on 2nd half) before and after training adapter on first half of text')\n",
    "df = pd.DataFrame(data).set_index('name')\n",
    "\n",
    "df['learning'] = (df['before']-df['after'])/df['before']\n",
    "df.sort_values('learning').drop(columns=['text', 'url'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
