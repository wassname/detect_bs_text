{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "An attempt to measure suprise in text using adapters\n",
    "\n",
    "https://github.com/huggingface/peft/blob/main/examples/fp4_finetuning/finetune_fp4_opt_bnb_peft.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "# from peft import LoraConfig, get_peft_model\n",
    "\n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../samples/2024_anthropic_palintir.md\n",
      "../samples/2024_arxiv_meh.md\n",
      "../samples/2024_bob_fanfic.md\n",
      "../samples/2024_bob_fanfic2.md\n",
      "../samples/2024_deliberative_alignment.md\n",
      "../samples/2024_gpt4_fake_paper.md\n",
      "../samples/2024_gwern_reddit.md\n",
      "../samples/2024_how_to_focus.md\n",
      "../samples/2024_lesswrong_slop.md\n",
      "../samples/2024_news_anthropic.md\n",
      "../samples/2024_openai_emails.md\n",
      "../samples/2024_trump_appointment.md\n",
      "../samples/2025_h5n1_report.md\n",
      "../samples/2025_lw_human-study-on-ai-spear-phishing-campaigns.md\n",
      "../samples/2025_lw_parkinson-s-law-and-the-ideology-of-statistics-1.md\n",
      "../samples/2025_lw_the-intelligence-curse.md\n",
      "../samples/2025_lw_the-laws-of-large-numbers.md\n",
      "../samples/2025_lw_what-s-the-short-timeline-plan.md\n",
      "../samples/lorem_ipsum.md\n",
      "../samples/politics_is_the_mind_killer.md\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>f</th>\n",
       "      <th>content</th>\n",
       "      <th>url</th>\n",
       "      <th>novelty</th>\n",
       "      <th>date</th>\n",
       "      <th>in_training</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Anthropic and Palantir Partner to Bring Claude...</td>\n",
       "      <td>../samples/2024_anthropic_palintir.md</td>\n",
       "      <td>Anthropic and Palantir Technologies Inc. (NYSE...</td>\n",
       "      <td>https://investors.palantir.com/news-details/20...</td>\n",
       "      <td>0.20</td>\n",
       "      <td>2024-07-11 00:00:00+00:00</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TradingAgents: Multi-Agents LLM Financial Trad...</td>\n",
       "      <td>../samples/2024_arxiv_meh.md</td>\n",
       "      <td>TradingAgents: Multi-Agents LLM Financial Trad...</td>\n",
       "      <td>https://arxiv.org/html/2412.20138v1</td>\n",
       "      <td>0.15</td>\n",
       "      <td>2024-12-28 00:00:00+00:00</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Anthropic and Palantir Partner to Bring Claude...   \n",
       "1  TradingAgents: Multi-Agents LLM Financial Trad...   \n",
       "\n",
       "                                       f  \\\n",
       "0  ../samples/2024_anthropic_palintir.md   \n",
       "1           ../samples/2024_arxiv_meh.md   \n",
       "\n",
       "                                             content  \\\n",
       "0  Anthropic and Palantir Technologies Inc. (NYSE...   \n",
       "1  TradingAgents: Multi-Agents LLM Financial Trad...   \n",
       "\n",
       "                                                 url  novelty  \\\n",
       "0  https://investors.palantir.com/news-details/20...     0.20   \n",
       "1                https://arxiv.org/html/2412.20138v1     0.15   \n",
       "\n",
       "                       date  in_training  \n",
       "0 2024-07-11 00:00:00+00:00        False  \n",
       "1 2024-12-28 00:00:00+00:00        False  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import frontmatter\n",
    "from pathlib import Path\n",
    "from loguru import logger\n",
    "\n",
    "sample_files = sorted(Path(\"../samples/\").glob('*.md'))\n",
    "# print(sample_files)\n",
    "\n",
    "max_chars = 2000\n",
    "max_summary_frac = 0.1\n",
    "samples = []\n",
    "for f in sample_files:\n",
    "    print(f)\n",
    "    with open(f, \"r\") as file:\n",
    "        post = frontmatter.load(file)\n",
    "        samples.append(dict(content=post.content[:max_chars], f=f, **post.metadata))\n",
    "\n",
    "df = pd.DataFrame(samples)\n",
    "df = df[['title', 'f', 'content', 'url', 'novelty', 'date']]\n",
    "df['date'] = pd.to_datetime(df['date'], utc=True)\n",
    "df['in_training'] = df.date < '2024-01-01'\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TradingAgents framework uses multi-agent LLM for financial trading, mimicking real trading dynamics. Roles include analysts and traders with diverse risk profiles. Key features include market conditio\n",
      "n assessors, risk management, and decision-making. Superior to baseline models, improves returns, Sharpe ratio, max drawdown.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from anycache import anycache\n",
    "\n",
    "cache_file = \"../.anycache\"\n",
    "\n",
    "# to clear\n",
    "import shutil\n",
    "shutil.rmtree(cache_file, ignore_errors=True)\n",
    "\n",
    "@anycache(cachedir=cache_file)\n",
    "def summize_gpt4(text):\n",
    "    client = OpenAI()\n",
    "    content = f\"Make a tl;dr of this text in <280 chars.\\n\\n## Text\\n\\n{text}\\n\\n## Instruction\\n\\nMake a tl;dr of this text in <280 chars. Start with the most important, as extra text will be discarded :\\n\\ntl;dr:\"\n",
    "    chat_completion = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": content,\n",
    "                }\n",
    "            ],\n",
    "            model=\"gpt-4\",\n",
    "        )\n",
    "    r = chat_completion.choices[0].message.content\n",
    "    return r\n",
    "\n",
    "@anycache(cachedir=cache_file)\n",
    "def summarize_gpt4b(text):\n",
    "    l = int(len(text)*max_summary_frac)\n",
    "    # print(l)\n",
    "    client = OpenAI()\n",
    "    inst = \"We aim to compress then reconstruct a text. First lets do the compression. In short hand, record the information needed to reconstruct the text (type of document, writing style, suprising contenxt, etc). You have <{l} chars. Start with the most important, as extra text will be discarded\"\n",
    "    content = f\"{inst}\\n\\n## Text\\n\\n{text}\\n\\n## Instruction\\n\\n{inst}:\\n\\ntl;dr:\"\n",
    "    chat_completion = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": content,\n",
    "                }\n",
    "            ],\n",
    "            model=\"gpt-4\",\n",
    "        )\n",
    "    # print(content)\n",
    "    r = chat_completion.choices[0].message.content\n",
    "    return r[:l], r[l:]\n",
    "\n",
    "r, _ = summarize_gpt4b(samples[1][\"content\"])\n",
    "print(r), print(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TradingAgents introduces a multi-agent financial trading framework using large language models. Agents specialize in roles like risk analysis and market condition evaluation, improving trading performance and showing potential for LLM frameworks in finance.\n"
     ]
    }
   ],
   "source": [
    "r = summize_gpt4(samples[1][\"content\"])\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs_writing_detector.metrics.ppx import perplexity_compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForMaskedLM, AutoModelForCausalLM, AutoConfig, PreTrainedTokenizerBase, PreTrainedTokenizer, GPTQConfig, BitsAndBytesConfig\n",
    "\n",
    "def load_model(model_name):\n",
    "    trust_remote_code = True\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=trust_remote_code)\n",
    "    config = AutoConfig.from_pretrained(model_name, trust_remote_code=trust_remote_code)\n",
    "    # print(config)\n",
    "    if config.quantization_config is not None:\n",
    "        config.quantization_config['disable_exllama'] = True\n",
    "        if 'use_exllama' in config.quantization_config:\n",
    "            del config.quantization_config['use_exllama']\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=trust_remote_code, \n",
    "                                                 config=config,\n",
    "                                                 )\n",
    "    return model, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "def clear_mem():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    # \"TheBloke/phi-2-GPTQ\",\n",
    "    # \"TheBloke/Llama-2-7B-GPTQ\",\n",
    "    # \"TheBloke/Llama-2-13B-GPTQ\",\n",
    "    # \"TheBloke/Mistral-7B-v0.1-GPTQ\",\n",
    "    #  \"unsloth/Llama-3.2-1B\",\n",
    "     \"unsloth/Llama-3.2-1B-bnb-4bit\",\n",
    "]\n",
    "# model_name = \"unsloth/Llama-3.2-1B\"\n",
    "# model_name = \"unsloth/Llama-3.2-1B-bnb-4bit\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from bs_writing_detector.metrics.ppx import perplexity_compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method', 'disable_exllama']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unsloth/Llama-3.2-1B-bnb-4bit\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d28b339f7c3495c97adef2c4f1938ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.731894616981088 2.464229501783848 | Anthropic and Palantir Partner to Bring Claude AI Models to AWS for U.S. Government Intelligence and Defense Operations\n",
      "3.018628165803172 2.8031317269160456 | TradingAgents: Multi-Agents LLM Financial Trading Framework\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "summaries = {}\n",
    "data = []\n",
    "for model_name in models:\n",
    "    model, tokenizer = load_model(model_name)\n",
    "    print(model_name)\n",
    "    for i in tqdm(range(len(df))):\n",
    "        sample = df.iloc[i]\n",
    "        if sample['title'] not in summaries:\n",
    "            summaries[sample['title']] = summarize_gpt4b(sample['content'])[0]\n",
    "        summary = summaries[sample['title']]\n",
    "\n",
    "        # before \n",
    "        s1 = sample['content']\n",
    "        before = perplexity_compute(data=s1, model=model, tokenizer=tokenizer, device='cuda')['nlls'][0]\n",
    "\n",
    "        n_tokens = len(tokenizer(s1)['input_ids'])\n",
    "\n",
    "        # after \n",
    "        s2 = f\"\"\"\n",
    "        High level summary: {summary}\n",
    "\n",
    "Text:\n",
    "{sample['content']}\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO add attn mask\n",
    "        after = perplexity_compute(data=s2, model=model, tokenizer=tokenizer, device='cuda')['nlls'][0][-n_tokens:]\n",
    "\n",
    "        print(f\"{before.mean()} {after.mean()} | {sample['title']}\")\n",
    "        data.append(dict(\n",
    "            before_mean=before.mean(), \n",
    "            before_sum=before.sum().item(),\n",
    "            before_std=before.std(),\n",
    "            before_min=before.min(),\n",
    "            before_max=before.max(),\n",
    "            before_ppx=np.exp(before.mean().item()),\n",
    "                         after_mean=after.mean(),\n",
    "                         after_sum=after.sum().item(),\n",
    "                            after_std=after.std(),\n",
    "                            after_min=after.min(),\n",
    "                            after_max=after.max(), \n",
    "                            after_ppx=np.exp(after.mean().item()),\n",
    "                         model=model_name, \n",
    "                         summary=summary,\n",
    "                        #  sample=sample['title'],\n",
    "                        #  in_training=sample['in_training'], \n",
    "                         len=len(sample['content']),\n",
    "                         n_tokens=n_tokens,\n",
    "                         **sample\n",
    "                         ),\n",
    "\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results\n",
    "df2 = pd.DataFrame(data).set_index('title')\n",
    "df2 = df2.query('in_training == False')\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QC summary\n",
    "df2.summary.str.len().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n,d in df2.groupby(\"model\"):\n",
    "    for stat in ['mean', 'std', 'min', 'max']:\n",
    "        df2[f\"improvement%_{stat}\"] = (df2[f\"before_{stat}\"] - df2[f\"after_{stat}\"]) / df2[f\"before_{stat}\"]\n",
    "        df2[f\"improvement_{stat}\"] = (df2[f\"before_{stat}\"] - df2[f\"after_{stat}\"])\n",
    "        df2[f\"summarizable_{stat}\"] = df2[f\"improvement_{stat}\"]  > 1\n",
    "        df2[f\"summarizable2_{stat}\"] = df2[f\"improvement%_{stat}\"]  > 0.05\n",
    "        df2[f'suprising_{stat}'] = df2[f\"before_{stat}\"] > 15\n",
    "        df2[f'BS_{stat}'] = ~df2[f\"summarizable_{stat}\"] | ~df2[f'suprising_{stat}']\n",
    "\n",
    "\n",
    "    print(n)\n",
    "    # d = d[[ 'before', 'after', \"improvement\", \"improvement%\", 'suprising', 'summarizable', 'summarizable2', 'novelty' ]].sort_values(\"improvement%\", ascending=True)\n",
    "    # print(d.to_markdown())\n",
    "    # display(d)\n",
    "\n",
    "    # TODO turn into a single metric, correlate with novelty label\n",
    "    r = df2.select_dtypes(include=np.number).corr()['novelty'].abs().sort_values()\n",
    "    display(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.select_dtypes(include=np.number).corr()['novelty'].sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
